{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarryMacFarlane/AI-Course-Work/blob/main/Mini_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbrnEd1SRe4v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "vtgSxjqV8c0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network Deciscion Tree (Code from Paper)\n",
        "+ Including a class to easily run it"
      ],
      "metadata": {
        "id": "XZ62jX31s88o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "\n",
        "def tf_kron_prod(a, b):\n",
        "    res = tf.einsum('ij,ik->ijk', a, b)\n",
        "    res = tf.reshape(res, [-1, tf.reduce_prod(res.shape[1:])])\n",
        "    return res\n",
        "\n",
        "\n",
        "def tf_bin(x, cut_points, temperature=0.1):\n",
        "    # x is a N-by-1 matrix (column vector)\n",
        "    # cut_points is a D-dim vector (D is the number of cut-points)\n",
        "    # this function produces a N-by-(D+1) matrix, each row has only one element being one and the rest are all zeros\n",
        "    D = cut_points.get_shape().as_list()[0]\n",
        "    W = tf.reshape(tf.linspace(1.0, D + 1.0, D + 1), [1, -1])\n",
        "    cut_points = tf.sort(cut_points)  # make sure cut_points is monotonically increasing\n",
        "    b = tf.cumsum(tf.concat([tf.constant(0.0, shape=[1]), -cut_points], 0))\n",
        "    h = tf.matmul(x, W) + b\n",
        "    res = tf.nn.softmax(h / temperature)\n",
        "    return res\n",
        "\n",
        "\n",
        "def nn_decision_tree(x, cut_points_list, leaf_score, temperature=0.1):\n",
        "    # cut_points_list contains the cut_points for each dimension of feature\n",
        "    leaf = reduce(tf_kron_prod,\n",
        "                  map(lambda z: tf_bin(x[:, z[0]:z[0] + 1], z[1], temperature), enumerate(cut_points_list)))\n",
        "    return tf.matmul(leaf, leaf_score)\n",
        "\n",
        "\n",
        "\n",
        "class DNDT():\n",
        "    #Class for making deep neural decision trees\n",
        "\n",
        "    def __init__(self, num_classes, num_features, temperature, learning_rate, iters, epsilon):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_cut = [1]*num_features\n",
        "        self.cutpoint_list = [tf.Variable(tf.random.uniform([i])) for i in self.num_cut]\n",
        "        num_leaf = np.prod(np.array(self.num_cut) + 1)\n",
        "        self.leafscore = tf.Variable(tf.random.uniform([num_leaf, self.num_classes]))\n",
        "        self.temp = temperature\n",
        "        self.lr = learning_rate\n",
        "        self.epoch = iters\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_test, y_test):\n",
        "      sess = tf.compat.v1.InteractiveSession()\n",
        "      sess.run(tf.compat.v1.disable_v2_behavior())\n",
        "      x_ph = tf.compat.v1.placeholder(tf.float32, [None, X.shape[1]])\n",
        "      y_ph = tf.compat.v1.placeholder(tf.float32, [None, self.num_classes])\n",
        "      y_pred = nn_decision_tree(x_ph, self.cutpoint_list, self.leafscore, temperature=0.1)\n",
        "      loss = tf.reduce_mean(tf.compat.v1.losses.softmax_cross_entropy(logits=y_pred, onehot_labels=y_ph))\n",
        "      opt = tf.compat.v1.train.AdamOptimizer(self.lr)\n",
        "      train_step = opt.minimize(loss)\n",
        "      sess.run(tf.compat.v1.global_variables_initializer())\n",
        "      tf.compat.v1.initialize_all_variables\n",
        "      j = 0\n",
        "      for i in range(self.epoch):\n",
        "       #I think first element returned is accuracy, but I'm not 100 % sure\n",
        "        _, loss_e = sess.run(fetches = [train_step, loss], feed_dict={x_ph: X, y_ph: y})\n",
        "        if loss_e < self.epsilon:\n",
        "          print(i, \" epochs: \", loss_e)\n",
        "          break\n",
        "        elif (i % (self.epoch/4)) == 0:\n",
        "          print(j*25,\"%\")\n",
        "          print(\"Loss: \", loss_e)\n",
        "          pred = y_pred.eval(feed_dict={x_ph: X, y_ph: y})\n",
        "          print(\"Accuracy: \", self.accuracy(pred, y))\n",
        "          print(\"\\n\")\n",
        "          j = j + 1\n",
        "      pred = y_pred.eval(feed_dict={x_ph: X_test, y_ph: y_test})\n",
        "      print(\"Final Test Accuracy: \", self.accuracy(pred, y_test))\n",
        "      self.tree = y_pred\n",
        "      self.sess = sess\n",
        "\n",
        "\n",
        "    def closeSession(self):\n",
        "      self.sess.close()\n",
        "\n",
        "    def accuracy(self, y_pred, y_true):\n",
        "      length = len(y_pred)\n",
        "      acc = 0\n",
        "      for i in range(length):\n",
        "        current = [0]*2\n",
        "        for c in range(self.num_classes):\n",
        "          if current[0] < y_pred[i][c]:\n",
        "            current[0] = y_pred[i][c]\n",
        "            current[1] = c\n",
        "        if y_true[i][current[1]] == 1:\n",
        "          acc = acc + 1\n",
        "\n",
        "      return (acc/length)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Br7djbSs8j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network"
      ],
      "metadata": {
        "id": "IGNuJVA78gvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "  def __init__(self, num_layers, units_layer, input_shapeX, dropout, num_classes, flatten = False):\n",
        "    #num_layers : int -> How many layers in the neural network\n",
        "    #units_layer : int -> How many units per layer\n",
        "    #input_shapeX : int or tuple (int,int, ...) -> What is the shape of the input (if this is a tuple, then flatten must be True)\n",
        "    #dropout : int (b/w 0 and 1) -> What percentage of units are set to 0 during fitting (prevents overfitting)\n",
        "    if flatten:\n",
        "      self.model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape= input_shapeX),\n",
        "        tf.keras.layers.Input(shape=input_shapeX),\n",
        "  tf.keras.layers.Dense(units_layer[0], activation='relu')\n",
        "  ])\n",
        "    else:\n",
        "      self.model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=input_shapeX),\n",
        "  tf.keras.layers.Dense(units_layer[0], activation='relu', input_dim = input_shapeX)\n",
        "  ])\n",
        "    for i in range(num_layers-1):\n",
        "      if i == 0:\n",
        "        continue\n",
        "      self.model.add(tf.keras.layers.Dense(units_layer[i], activation = 'relu'))\n",
        "    self.model.add(tf.keras.layers.Dropout(dropout))\n",
        "    self.model.add(tf.keras.layers.Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "    #We will just use Adam and a loss function provided by tensorflow\n",
        "    if num_classes == 2:\n",
        "      loss_fn = 'binary_crossentropy'\n",
        "    else:\n",
        "      loss_fn = 'categorical_crossentropy'\n",
        "    self.model.compile(optimizer='adam',\n",
        "              loss= loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "    return\n",
        "\n",
        "  #Method fits model up to certain iteration\n",
        "  def fit(self, x_train, y_train, iter, print):\n",
        "    if print:\n",
        "      self.model.fit(x_train, y_train, epochs=iter)\n",
        "    else:\n",
        "      self.model.fit(x_train, y_train, epochs=iter, verbose=0)\n",
        "    return\n",
        "\n",
        "  #Method fits model and saves info on validation set (size of which is decided by ratio) and frequency determined by freq\n",
        "  def validationfitting(self, x_train, y_train, X_val, y_val, print, iter = 30):\n",
        "    if print:\n",
        "      self.model.fit(x_train, y_train, epochs=iter, validation_data=(X_val, y_val), verbose = 1)\n",
        "    else:\n",
        "      self.model.fit(x_train, y_train, epochs=iter, validation_data=(X_val, y_val), verbose = 0)\n",
        "    return\n",
        "  def test(self, x_test, y_test):\n",
        "    stats = self.model.evaluate(x_test,  y_test, verbose=0)\n",
        "    return stats\n",
        "\n"
      ],
      "metadata": {
        "id": "EDwbfEkL8kmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Decision Tree"
      ],
      "metadata": {
        "id": "g-0SRy2VQQa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, classification_report\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NOlS29TnQSbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "8xs3SLX7be3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read me"
      ],
      "metadata": {
        "id": "UKgsN7QplmlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the following codes in order, please upload the data files to the session storage.\n",
        "This can be done in two step:\n",
        "  1. Click on the folder icon on the left sidebar.\n",
        "  2. Click on the upload icon (an arrow pointing up) and select the CSV files.\n",
        "  \n",
        "Files required will be provided in a zip file, including:\n",
        "  1. titanic.csv\n",
        "  2. haberman.data\n",
        "  3. diabetes.csv\n",
        "  4. credit_training.csv\n",
        "  5. credit_test.csv\n",
        "  6. poker-hand-training-true.data\n",
        "  7. poker-hand-testing.data\n",
        "\n",
        "The dataset are processed to either pandas frame or python list. Please use them according to your need."
      ],
      "metadata": {
        "id": "GqVcgvTaltpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up"
      ],
      "metadata": {
        "id": "qsC55pwnlyHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbt54EIGd3Zt",
        "outputId": "8bc9cbfe-b42f-4923-d301-3073533f5599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bowchYxWerLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uciData(path, column_names, y_index=-1):\n",
        "    df = pd.read_csv(path, header=None, names=column_names)\n",
        "    aList = df.values.tolist()\n",
        "    #a = np.array(aList)\n",
        "\n",
        "    a_x = []\n",
        "    a_y = []\n",
        "    if y_index == -1:\n",
        "        for instance in aList:\n",
        "            a_x.append(instance[:-1])\n",
        "            a_y.append(instance[-1])\n",
        "    else:\n",
        "        for instance in aList:\n",
        "            a_x.append(instance[:y_index] + instance[y_index+1:])\n",
        "            a_y.append(instance[y_index])\n",
        "\n",
        "    return a_x, a_y"
      ],
      "metadata": {
        "id": "Y_iwaFdEeVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kagData(path, y_index=-1):\n",
        "    df = pd.read_csv(path)\n",
        "    aList = df.values.tolist()\n",
        "\n",
        "    a_x = []\n",
        "    a_y = []\n",
        "    if y_index == -1:\n",
        "        for instance in aList:\n",
        "            a_x.append(instance[:-1])\n",
        "            a_y.append(instance[-1])\n",
        "    else:\n",
        "        for instance in aList:\n",
        "            a_x.append(instance[:y_index] + instance[y_index+1:])\n",
        "            a_y.append(instance[y_index])\n",
        "\n",
        "    return a_x, a_y"
      ],
      "metadata": {
        "id": "eSGGipbsf7dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "z-Fkn9Hcl_k2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iris"
      ],
      "metadata": {
        "id": "iOM-bzFNdvld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# fetch dataset\n",
        "iris = fetch_ucirepo(id=53)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_iris = iris.data.features\n",
        "Y_iris = iris.data.targets\n",
        "\n",
        "# data (as numpy array)\n",
        "X_iris = X_iris.to_numpy()\n",
        "Y_iris = Y_iris.to_numpy()\n",
        "\n",
        "#One-hot encoding Y\n",
        "y_enc = LabelEncoder().fit_transform(Y_iris)\n",
        "y_label = tf.keras.utils.to_categorical(y_enc) #Converting the label into a matrix form\n",
        "\n",
        "# metadata\n",
        "print(iris.metadata)\n",
        "\n",
        "# variable information\n",
        "print(iris.variables)\n",
        "\n",
        "# Split into train and test\n",
        "Iris_X_train, Iris_X_test, Iris_y_train, Iris_y_test = train_test_split(X_iris, y_label, test_size=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of3qm3crbk-O",
        "outputId": "ae1e6b01-013f-4ebc-aaab-3c8898455174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'published_in': 'Significance, 2021', 'year': 2021, 'url': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'doi': '1740-9713.01589'}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n",
            "           name     role         type demographic  \\\n",
            "0  sepal length  Feature   Continuous        None   \n",
            "1   sepal width  Feature   Continuous        None   \n",
            "2  petal length  Feature   Continuous        None   \n",
            "3   petal width  Feature   Continuous        None   \n",
            "4         class   Target  Categorical        None   \n",
            "\n",
            "                                         description units missing_values  \n",
            "0                                               None    cm             no  \n",
            "1                                               None    cm             no  \n",
            "2                                               None    cm             no  \n",
            "3                                               None    cm             no  \n",
            "4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Haberman's Survival"
      ],
      "metadata": {
        "id": "rnEnTZHFeMBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "path = \"/content/haberman.data\"\n",
        "names = [\"Age\", \"Year\", \"Nodes\", \"Survival_Status\"]\n",
        "X_haberman, y_haberman = uciData(path, names)\n",
        "X_haberman = np.array(X_haberman)\n",
        "y_haberman = np.array(y_haberman)\n",
        "\n",
        "#One-hot encoding Y\n",
        "y_enc = LabelEncoder().fit_transform(y_haberman)\n",
        "y_label = tf.keras.utils.to_categorical(y_enc) #Converting the label into a matrix form\n",
        "\n",
        "#Rename and train/test split\n",
        "Haber_X_train, Haber_X_test, Haber_y_train, Haber_y_test = train_test_split(X_haberman, y_label, test_size=0.3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cq1582XIexKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Car Evaluation"
      ],
      "metadata": {
        "id": "o2pcJmaWfImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "car_evaluation = fetch_ucirepo(id=19)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_car = car_evaluation.data.features\n",
        "y_car = car_evaluation.data.targets\n",
        "\n",
        "#Make them numpy array's\n",
        "X_car = np.array(X_car)\n",
        "y_car = np.array(y_car)\n",
        "\n",
        "# metadata\n",
        "print(car_evaluation.metadata)\n",
        "\n",
        "# variable information\n",
        "print(car_evaluation.variables)\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_car)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "Car_X_train, Car_X_test, Car_y_train, Car_y_test = train_test_split(X_car, y_label, test_size=0.3)\n",
        "\n",
        "#We need to encode the data into onehot features, for everything, I will do this tomorrow, for now we don't use this\n",
        "print(Car_X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_lG0azLfImY",
        "outputId": "ee6bb508-c283-4c51-ffb9-287d13a6cd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 19, 'name': 'Car Evaluation', 'repository_url': 'https://archive.ics.uci.edu/dataset/19/car+evaluation', 'data_url': 'https://archive.ics.uci.edu/static/public/19/data.csv', 'abstract': 'Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods.', 'area': 'Other', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1728, 'num_features': 6, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1988, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5JP48', 'creators': ['Marko Bohanec'], 'intro_paper': {'title': 'Knowledge acquisition and explanation for multi-attribute decision making', 'authors': 'M. Bohanec, V. Rajkoviƒç', 'published_in': '8th Intl Workshop on Expert Systems and their Applications, Avignon, France', 'year': 1988, 'url': 'https://www.semanticscholar.org/paper/KNOWLEDGE-ACQUISITION-AND-EXPLANATION-FOR-DECISION-Bohanec-Rajkovi%C4%8D/8bab443ae322ff47c3e609272bd93fd4650555bc', 'doi': None}, 'additional_info': {'summary': 'Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\\r\\n\\r\\nCAR                      car acceptability\\r\\n. PRICE                  overall price\\r\\n. . buying               buying price\\r\\n. . maint                price of the maintenance\\r\\n. TECH                   technical characteristics\\r\\n. . COMFORT              comfort\\r\\n. . . doors              number of doors\\r\\n. . . persons            capacity in terms of persons to carry\\r\\n. . . lug_boot           the size of luggage boot\\r\\n. . safety               estimated safety of the car\\r\\n\\r\\nInput attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\\r\\n\\r\\nThe Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\\r\\n\\r\\nBecause of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'buying:   vhigh, high, med, low.\\nmaint:    vhigh, high, med, low.\\ndoors:    2, 3, 4, 5more.\\npersons:  2, 4, more.\\nlug_boot: small, med, big.\\nsafety:   low, med, high.', 'citation': None}}\n",
            "       name     role         type demographic  \\\n",
            "0    buying  Feature  Categorical        None   \n",
            "1     maint  Feature  Categorical        None   \n",
            "2     doors  Feature  Categorical        None   \n",
            "3   persons  Feature  Categorical        None   \n",
            "4  lug_boot  Feature  Categorical        None   \n",
            "5    safety  Feature  Categorical        None   \n",
            "6     class   Target  Categorical        None   \n",
            "\n",
            "                                         description units missing_values  \n",
            "0                                       buying price  None             no  \n",
            "1                           price of the maintenance  None             no  \n",
            "2                                    number of doors  None             no  \n",
            "3              capacity in terms of persons to carry  None             no  \n",
            "4                           the size of luggage boot  None             no  \n",
            "5                        estimated safety of the car  None             no  \n",
            "6  evaulation level (unacceptable, acceptable, go...  None             no  \n",
            "['low' 'high' '2' '2' 'med' 'low']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Titanic *"
      ],
      "metadata": {
        "id": "kt5MQ-mXfrk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/titanic.csv\"\n",
        "X_titanic, y_titanic = kagData(path)\n",
        "\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_titanic)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Make numpy arrays\n",
        "X_titanic = np.array(X_titanic)\n",
        "y_label = np.array(y_label)\n",
        "#Rename and split into train and test\n",
        "Tit_X_train, Tit_X_test, Tit_y_train, Tit_y_test = train_test_split(X_titanic, y_label, test_size=0.3)\n",
        "print(len(Tit_X_train[0])) #input 27\n",
        "print(len(Tit_y_train[0])) #output 2\n",
        "print(Tit_X_train)"
      ],
      "metadata": {
        "id": "tNDmwviNfrk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de307d0-2dde-4f96-bf08-5cc728612f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "2\n",
            "[[350.      42.       8.6625 ...   2.       0.       0.    ]\n",
            " [339.      45.       8.05   ...   2.       0.       0.    ]\n",
            " [748.      30.      13.     ...   2.       0.       0.    ]\n",
            " ...\n",
            " [728.      28.       7.7375 ...   1.       0.       0.    ]\n",
            " [959.      47.      42.4    ...   2.       0.       0.    ]\n",
            " [596.      36.      24.15   ...   2.       0.       0.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breast Cancer Wisconsin"
      ],
      "metadata": {
        "id": "rsVaBiGFgqSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "breast_cancer_wisconsin_original = fetch_ucirepo(id=15)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_cancer = breast_cancer_wisconsin_original.data.features\n",
        "y_cancer = breast_cancer_wisconsin_original.data.targets\n",
        "\n",
        "# metadata\n",
        "print(breast_cancer_wisconsin_original.metadata)\n",
        "\n",
        "# variable information\n",
        "print(breast_cancer_wisconsin_original.variables)\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_cancer)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "Cancer_X_train, Cancer_X_test, Cancer_y_train, Cancer_y_test = train_test_split(X_cancer, y_label, test_size=0.3)\n",
        "print(Cancer_X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGyOd_llgqSv",
        "outputId": "5a25e248-b435-4719-8d30-957fb9944cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 15, 'name': 'Breast Cancer Wisconsin (Original)', 'repository_url': 'https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original', 'data_url': 'https://archive.ics.uci.edu/static/public/15/data.csv', 'abstract': 'Original Wisconsin Breast Cancer Database', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 699, 'num_features': 9, 'feature_types': ['Integer'], 'demographics': [], 'target_col': ['Class'], 'index_col': ['Sample_code_number'], 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1990, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5HP4Z', 'creators': ['WIlliam Wolberg'], 'intro_paper': None, 'additional_info': {'summary': \"Samples arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore reflects this chronological grouping of the data. This grouping information appears immediately below, having been removed from the data itself:\\r\\n\\r\\nGroup 1: 367 instances (January 1989)\\r\\nGroup 2:  70 instances (October 1989)\\r\\nGroup 3:  31 instances (February 1990)\\r\\nGroup 4:  17 instances (April 1990)\\r\\nGroup 5:  48 instances (August 1990)\\r\\nGroup 6:  49 instances (Updated January 1991)\\r\\nGroup 7:  31 instances (June 1991)\\r\\nGroup 8:  86 instances (November 1991)\\r\\n-----------------------------------------\\r\\nTotal:   699 points (as of the donated datbase on 15 July 1992)\\r\\n\\r\\nNote that the results summarized above in Past Usage refer to a dataset of size 369, while Group 1 has only 367 instances.  This is because it originally contained 369 instances; 2 were removed.  The following statements summarizes changes to the original Group 1's set of data:\\r\\n\\r\\n#####  Group 1 : 367 points: 200B 167M (January 1989)\\r\\n\\r\\n#####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\\r\\n\\r\\n#####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\\r\\n#####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\\r\\n#####                  : Changed 0 to 1 in field 6 of sample 1219406\\r\\n#####                  : Changed 0 to 1 in field 8 of following sample:\\r\\n#####                  : 1182404,2,3,1,1,1,2,0,1,1,1\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1. Sample code number:            id number\\r\\n2. Clump Thickness:               1 - 10\\r\\n3. Uniformity of Cell Size:       1 - 10\\r\\n4. Uniformity of Cell Shape:      1 - 10\\r\\n5. Marginal Adhesion:             1 - 10\\r\\n6. Single Epithelial Cell Size:   1 - 10\\r\\n7. Bare Nuclei:                   1 - 10\\r\\n8. Bland Chromatin:               1 - 10\\r\\n9. Normal Nucleoli:               1 - 10\\r\\n10. Mitoses:                       1 - 10\\r\\n11. Class:                        (2 for benign, 4 for malignant)', 'citation': None}}\n",
            "                           name     role         type demographic  \\\n",
            "0            Sample_code_number       ID  Categorical        None   \n",
            "1               Clump_thickness  Feature      Integer        None   \n",
            "2       Uniformity_of_cell_size  Feature      Integer        None   \n",
            "3      Uniformity_of_cell_shape  Feature      Integer        None   \n",
            "4             Marginal_adhesion  Feature      Integer        None   \n",
            "5   Single_epithelial_cell_size  Feature      Integer        None   \n",
            "6                   Bare_nuclei  Feature      Integer        None   \n",
            "7               Bland_chromatin  Feature      Integer        None   \n",
            "8               Normal_nucleoli  Feature      Integer        None   \n",
            "9                       Mitoses  Feature      Integer        None   \n",
            "10                        Class   Target       Binary        None   \n",
            "\n",
            "                  description units missing_values  \n",
            "0                        None  None             no  \n",
            "1                        None  None             no  \n",
            "2                        None  None             no  \n",
            "3                        None  None             no  \n",
            "4                        None  None             no  \n",
            "5                        None  None             no  \n",
            "6                        None  None            yes  \n",
            "7                        None  None             no  \n",
            "8                        None  None             no  \n",
            "9                        None  None             no  \n",
            "10  2 = benign, 4 = malignant  None             no  \n",
            "     Clump_thickness  Uniformity_of_cell_size  Uniformity_of_cell_shape  \\\n",
            "544                2                        1                         3   \n",
            "327                1                        1                         1   \n",
            "277                1                        1                         1   \n",
            "153                4                        1                         1   \n",
            "12                 5                        3                         3   \n",
            "..               ...                      ...                       ...   \n",
            "615                4                        1                         3   \n",
            "75                 1                        1                         2   \n",
            "503                4                        1                         1   \n",
            "273                7                        2                         4   \n",
            "138                4                        1                         2   \n",
            "\n",
            "     Marginal_adhesion  Single_epithelial_cell_size  Bare_nuclei  \\\n",
            "544                  2                            2          1.0   \n",
            "327                  1                            2          1.0   \n",
            "277                  1                            2          1.0   \n",
            "153                  1                            2          3.0   \n",
            "12                   3                            2          3.0   \n",
            "..                 ...                          ...          ...   \n",
            "615                  1                            2          1.0   \n",
            "75                   1                            2          2.0   \n",
            "503                  1                            2          1.0   \n",
            "273                  1                            3          4.0   \n",
            "138                  1                            2          1.0   \n",
            "\n",
            "     Bland_chromatin  Normal_nucleoli  Mitoses  \n",
            "544                2                1        1  \n",
            "327                2                1        1  \n",
            "277                2                1        1  \n",
            "153                1                1        1  \n",
            "12                 4                4        1  \n",
            "..               ...              ...      ...  \n",
            "615                2                1        1  \n",
            "75                 4                2        1  \n",
            "503                3                1        1  \n",
            "273                3                3        1  \n",
            "138                2                1        1  \n",
            "\n",
            "[489 rows x 9 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pima Indian Diabetes"
      ],
      "metadata": {
        "id": "vSebvOf9hBDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/diabetes.csv\"\n",
        "X_diabetes, y_diabetes = kagData(path)\n",
        "\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_diabetes)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "Dia_X_train, Dia_X_test, Dia_y_train, Dia_y_test = train_test_split(X_diabetes, y_label, test_size=0.3)\n",
        "print(Dia_X_train) # 8 inputs\n",
        "print(Dia_y_train) # 2 outputs"
      ],
      "metadata": {
        "id": "0PyhXqQqhBDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5853cfdc-7944-452a-dc83-6bea9f33e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.0, 83.0, 58.0, 31.0, 18.0, 34.3, 0.336, 25.0], [2.0, 157.0, 74.0, 35.0, 440.0, 39.4, 0.134, 30.0], [1.0, 143.0, 74.0, 22.0, 61.0, 26.2, 0.256, 21.0], [7.0, 103.0, 66.0, 32.0, 0.0, 39.1, 0.344, 31.0], [4.0, 144.0, 58.0, 28.0, 140.0, 29.5, 0.287, 37.0], [5.0, 189.0, 64.0, 33.0, 325.0, 31.2, 0.583, 29.0], [3.0, 128.0, 72.0, 25.0, 190.0, 32.4, 0.549, 27.0], [2.0, 90.0, 60.0, 0.0, 0.0, 23.5, 0.191, 25.0], [2.0, 120.0, 54.0, 0.0, 0.0, 26.8, 0.455, 27.0], [0.0, 129.0, 80.0, 0.0, 0.0, 31.2, 0.703, 29.0], [5.0, 77.0, 82.0, 41.0, 42.0, 35.8, 0.156, 35.0], [3.0, 169.0, 74.0, 19.0, 125.0, 29.9, 0.268, 31.0], [10.0, 162.0, 84.0, 0.0, 0.0, 27.7, 0.182, 54.0], [8.0, 124.0, 76.0, 24.0, 600.0, 28.7, 0.687, 52.0], [13.0, 104.0, 72.0, 0.0, 0.0, 31.2, 0.465, 38.0], [1.0, 157.0, 72.0, 21.0, 168.0, 25.6, 0.123, 24.0], [0.0, 84.0, 82.0, 31.0, 125.0, 38.2, 0.233, 23.0], [2.0, 125.0, 60.0, 20.0, 140.0, 33.8, 0.088, 31.0], [0.0, 152.0, 82.0, 39.0, 272.0, 41.5, 0.27, 27.0], [13.0, 145.0, 82.0, 19.0, 110.0, 22.2, 0.245, 57.0], [7.0, 136.0, 74.0, 26.0, 135.0, 26.0, 0.647, 51.0], [4.0, 158.0, 78.0, 0.0, 0.0, 32.9, 0.803, 31.0], [1.0, 90.0, 62.0, 12.0, 43.0, 27.2, 0.58, 24.0], [13.0, 153.0, 88.0, 37.0, 140.0, 40.6, 1.174, 39.0], [9.0, 112.0, 82.0, 32.0, 175.0, 34.2, 0.26, 36.0], [5.0, 158.0, 84.0, 41.0, 210.0, 39.4, 0.395, 29.0], [14.0, 175.0, 62.0, 30.0, 0.0, 33.6, 0.212, 38.0], [8.0, 120.0, 78.0, 0.0, 0.0, 25.0, 0.409, 64.0], [2.0, 100.0, 70.0, 52.0, 57.0, 40.5, 0.677, 25.0], [12.0, 100.0, 84.0, 33.0, 105.0, 30.0, 0.488, 46.0], [3.0, 158.0, 76.0, 36.0, 245.0, 31.6, 0.851, 28.0], [3.0, 130.0, 78.0, 23.0, 79.0, 28.4, 0.323, 34.0], [8.0, 196.0, 76.0, 29.0, 280.0, 37.5, 0.605, 57.0], [12.0, 92.0, 62.0, 7.0, 258.0, 27.6, 0.926, 44.0], [1.0, 91.0, 64.0, 24.0, 0.0, 29.2, 0.192, 21.0], [0.0, 177.0, 60.0, 29.0, 478.0, 34.6, 1.072, 21.0], [2.0, 118.0, 80.0, 0.0, 0.0, 42.9, 0.693, 21.0], [13.0, 129.0, 0.0, 30.0, 0.0, 39.9, 0.569, 44.0], [10.0, 133.0, 68.0, 0.0, 0.0, 27.0, 0.245, 36.0], [0.0, 126.0, 84.0, 29.0, 215.0, 30.7, 0.52, 24.0], [3.0, 123.0, 100.0, 35.0, 240.0, 57.3, 0.88, 22.0], [0.0, 57.0, 60.0, 0.0, 0.0, 21.7, 0.735, 67.0], [5.0, 104.0, 74.0, 0.0, 0.0, 28.8, 0.153, 48.0], [2.0, 122.0, 60.0, 18.0, 106.0, 29.8, 0.717, 22.0], [2.0, 100.0, 64.0, 23.0, 0.0, 29.7, 0.368, 21.0], [10.0, 68.0, 106.0, 23.0, 49.0, 35.5, 0.285, 47.0], [2.0, 146.0, 0.0, 0.0, 0.0, 27.5, 0.24, 28.0], [8.0, 74.0, 70.0, 40.0, 49.0, 35.3, 0.705, 39.0], [0.0, 93.0, 100.0, 39.0, 72.0, 43.4, 1.021, 35.0], [3.0, 173.0, 78.0, 39.0, 185.0, 33.8, 0.97, 31.0], [3.0, 115.0, 66.0, 39.0, 140.0, 38.1, 0.15, 28.0], [1.0, 88.0, 78.0, 29.0, 76.0, 32.0, 0.365, 29.0], [7.0, 109.0, 80.0, 31.0, 0.0, 35.9, 1.127, 43.0], [7.0, 124.0, 70.0, 33.0, 215.0, 25.5, 0.161, 37.0], [5.0, 95.0, 72.0, 33.0, 0.0, 37.7, 0.37, 27.0], [1.0, 88.0, 30.0, 42.0, 99.0, 55.0, 0.496, 26.0], [11.0, 135.0, 0.0, 0.0, 0.0, 52.3, 0.578, 40.0], [0.0, 93.0, 60.0, 0.0, 0.0, 35.3, 0.263, 25.0], [3.0, 130.0, 64.0, 0.0, 0.0, 23.1, 0.314, 22.0], [5.0, 103.0, 108.0, 37.0, 0.0, 39.2, 0.305, 65.0], [9.0, 119.0, 80.0, 35.0, 0.0, 29.0, 0.263, 29.0], [5.0, 147.0, 78.0, 0.0, 0.0, 33.7, 0.218, 65.0], [10.0, 101.0, 76.0, 48.0, 180.0, 32.9, 0.171, 63.0], [1.0, 122.0, 64.0, 32.0, 156.0, 35.1, 0.692, 30.0], [5.0, 114.0, 74.0, 0.0, 0.0, 24.9, 0.744, 57.0], [1.0, 0.0, 48.0, 20.0, 0.0, 24.7, 0.14, 22.0], [2.0, 84.0, 0.0, 0.0, 0.0, 0.0, 0.304, 21.0], [0.0, 181.0, 88.0, 44.0, 510.0, 43.3, 0.222, 26.0], [0.0, 108.0, 68.0, 20.0, 0.0, 27.3, 0.787, 32.0], [2.0, 134.0, 70.0, 0.0, 0.0, 28.9, 0.542, 23.0], [4.0, 115.0, 72.0, 0.0, 0.0, 28.9, 0.376, 46.0], [6.0, 102.0, 90.0, 39.0, 0.0, 35.7, 0.674, 28.0], [8.0, 179.0, 72.0, 42.0, 130.0, 32.7, 0.719, 36.0], [1.0, 97.0, 70.0, 40.0, 0.0, 38.1, 0.218, 30.0], [7.0, 129.0, 68.0, 49.0, 125.0, 38.5, 0.439, 43.0], [9.0, 124.0, 70.0, 33.0, 402.0, 35.4, 0.282, 34.0], [0.0, 113.0, 76.0, 0.0, 0.0, 33.3, 0.278, 23.0], [0.0, 161.0, 50.0, 0.0, 0.0, 21.9, 0.254, 65.0], [2.0, 108.0, 62.0, 32.0, 56.0, 25.2, 0.128, 21.0], [9.0, 112.0, 82.0, 24.0, 0.0, 28.2, 1.282, 50.0], [6.0, 96.0, 0.0, 0.0, 0.0, 23.7, 0.19, 28.0], [4.0, 146.0, 92.0, 0.0, 0.0, 31.2, 0.539, 61.0], [2.0, 93.0, 64.0, 32.0, 160.0, 38.0, 0.674, 23.0], [3.0, 99.0, 54.0, 19.0, 86.0, 25.6, 0.154, 24.0], [4.0, 148.0, 60.0, 27.0, 318.0, 30.9, 0.15, 29.0], [0.0, 137.0, 84.0, 27.0, 0.0, 27.3, 0.231, 59.0], [7.0, 160.0, 54.0, 32.0, 175.0, 30.5, 0.588, 39.0], [2.0, 109.0, 92.0, 0.0, 0.0, 42.7, 0.845, 54.0], [5.0, 78.0, 48.0, 0.0, 0.0, 33.7, 0.654, 25.0], [2.0, 99.0, 60.0, 17.0, 160.0, 36.6, 0.453, 21.0], [0.0, 100.0, 70.0, 26.0, 50.0, 30.8, 0.597, 21.0], [4.0, 84.0, 90.0, 23.0, 56.0, 39.5, 0.159, 25.0], [5.0, 155.0, 84.0, 44.0, 545.0, 38.7, 0.619, 34.0], [4.0, 109.0, 64.0, 44.0, 99.0, 34.8, 0.905, 26.0], [4.0, 90.0, 0.0, 0.0, 0.0, 28.0, 0.61, 31.0], [3.0, 129.0, 92.0, 49.0, 155.0, 36.4, 0.968, 32.0], [1.0, 107.0, 68.0, 19.0, 0.0, 26.5, 0.165, 24.0], [10.0, 129.0, 76.0, 28.0, 122.0, 35.9, 0.28, 39.0], [0.0, 141.0, 0.0, 0.0, 0.0, 42.4, 0.205, 29.0], [5.0, 85.0, 74.0, 22.0, 0.0, 29.0, 1.224, 32.0], [2.0, 114.0, 68.0, 22.0, 0.0, 28.7, 0.092, 25.0], [13.0, 152.0, 90.0, 33.0, 29.0, 26.8, 0.731, 43.0], [2.0, 92.0, 62.0, 28.0, 0.0, 31.6, 0.13, 24.0], [2.0, 122.0, 70.0, 27.0, 0.0, 36.8, 0.34, 27.0], [10.0, 179.0, 70.0, 0.0, 0.0, 35.1, 0.2, 37.0], [2.0, 99.0, 70.0, 16.0, 44.0, 20.4, 0.235, 27.0], [5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0], [11.0, 111.0, 84.0, 40.0, 0.0, 46.8, 0.925, 45.0], [6.0, 105.0, 80.0, 28.0, 0.0, 32.5, 0.878, 26.0], [1.0, 144.0, 82.0, 46.0, 180.0, 46.1, 0.335, 46.0], [4.0, 146.0, 85.0, 27.0, 100.0, 28.9, 0.189, 27.0], [7.0, 106.0, 60.0, 24.0, 0.0, 26.5, 0.296, 29.0], [6.0, 107.0, 88.0, 0.0, 0.0, 36.8, 0.727, 31.0], [2.0, 88.0, 74.0, 19.0, 53.0, 29.0, 0.229, 22.0], [2.0, 112.0, 66.0, 22.0, 0.0, 25.0, 0.307, 24.0], [9.0, 171.0, 110.0, 24.0, 240.0, 45.4, 0.721, 54.0], [2.0, 94.0, 76.0, 18.0, 66.0, 31.6, 0.649, 23.0], [8.0, 65.0, 72.0, 23.0, 0.0, 32.0, 0.6, 42.0], [4.0, 95.0, 64.0, 0.0, 0.0, 32.0, 0.161, 31.0], [0.0, 127.0, 80.0, 37.0, 210.0, 36.3, 0.804, 23.0], [6.0, 144.0, 72.0, 27.0, 228.0, 33.9, 0.255, 40.0], [1.0, 102.0, 74.0, 0.0, 0.0, 39.5, 0.293, 42.0], [8.0, 186.0, 90.0, 35.0, 225.0, 34.5, 0.423, 37.0], [2.0, 71.0, 70.0, 27.0, 0.0, 28.0, 0.586, 22.0], [0.0, 95.0, 64.0, 39.0, 105.0, 44.6, 0.366, 22.0], [1.0, 100.0, 66.0, 15.0, 56.0, 23.6, 0.666, 26.0], [0.0, 180.0, 66.0, 39.0, 0.0, 42.0, 1.893, 25.0], [2.0, 98.0, 60.0, 17.0, 120.0, 34.7, 0.198, 22.0], [1.0, 139.0, 62.0, 41.0, 480.0, 40.7, 0.536, 21.0], [0.0, 179.0, 90.0, 27.0, 0.0, 44.1, 0.686, 23.0], [4.0, 129.0, 60.0, 12.0, 231.0, 27.5, 0.527, 31.0], [4.0, 110.0, 92.0, 0.0, 0.0, 37.6, 0.191, 30.0], [5.0, 115.0, 76.0, 0.0, 0.0, 31.2, 0.343, 44.0], [3.0, 163.0, 70.0, 18.0, 105.0, 31.6, 0.268, 28.0], [0.0, 105.0, 64.0, 41.0, 142.0, 41.5, 0.173, 22.0], [2.0, 142.0, 82.0, 18.0, 64.0, 24.7, 0.761, 21.0], [1.0, 117.0, 60.0, 23.0, 106.0, 33.8, 0.466, 27.0], [10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.537, 34.0], [8.0, 126.0, 74.0, 38.0, 75.0, 25.9, 0.162, 39.0], [3.0, 80.0, 0.0, 0.0, 0.0, 0.0, 0.174, 22.0], [5.0, 136.0, 82.0, 0.0, 0.0, 0.0, 0.64, 69.0], [3.0, 191.0, 68.0, 15.0, 130.0, 30.9, 0.299, 34.0], [4.0, 120.0, 68.0, 0.0, 0.0, 29.6, 0.709, 34.0], [1.0, 96.0, 64.0, 27.0, 87.0, 33.2, 0.289, 21.0], [4.0, 114.0, 65.0, 0.0, 0.0, 21.9, 0.432, 37.0], [1.0, 87.0, 68.0, 34.0, 77.0, 37.6, 0.401, 24.0], [8.0, 155.0, 62.0, 26.0, 495.0, 34.0, 0.543, 46.0], [1.0, 111.0, 62.0, 13.0, 182.0, 24.0, 0.138, 23.0], [4.0, 171.0, 72.0, 0.0, 0.0, 43.6, 0.479, 26.0], [7.0, 105.0, 0.0, 0.0, 0.0, 0.0, 0.305, 24.0], [7.0, 168.0, 88.0, 42.0, 321.0, 38.2, 0.787, 40.0], [8.0, 118.0, 72.0, 19.0, 0.0, 23.1, 1.476, 46.0], [0.0, 125.0, 68.0, 0.0, 0.0, 24.7, 0.206, 21.0], [11.0, 103.0, 68.0, 40.0, 0.0, 46.2, 0.126, 42.0], [10.0, 75.0, 82.0, 0.0, 0.0, 33.3, 0.263, 38.0], [0.0, 104.0, 76.0, 0.0, 0.0, 18.4, 0.582, 27.0], [3.0, 129.0, 64.0, 29.0, 115.0, 26.4, 0.219, 28.0], [10.0, 101.0, 86.0, 37.0, 0.0, 45.6, 1.136, 38.0], [5.0, 187.0, 76.0, 27.0, 207.0, 43.6, 1.034, 53.0], [4.0, 173.0, 70.0, 14.0, 168.0, 29.7, 0.361, 33.0], [4.0, 123.0, 80.0, 15.0, 176.0, 32.0, 0.443, 34.0], [6.0, 102.0, 82.0, 0.0, 0.0, 30.8, 0.18, 36.0], [1.0, 122.0, 90.0, 51.0, 220.0, 49.7, 0.325, 31.0], [4.0, 114.0, 64.0, 0.0, 0.0, 28.9, 0.126, 24.0], [3.0, 99.0, 62.0, 19.0, 74.0, 21.8, 0.279, 26.0], [2.0, 95.0, 54.0, 14.0, 88.0, 26.1, 0.748, 22.0], [0.0, 84.0, 64.0, 22.0, 66.0, 35.8, 0.545, 21.0], [2.0, 90.0, 68.0, 42.0, 0.0, 38.2, 0.503, 27.0], [5.0, 136.0, 84.0, 41.0, 88.0, 35.0, 0.286, 35.0], [3.0, 107.0, 62.0, 13.0, 48.0, 22.9, 0.678, 23.0], [2.0, 115.0, 64.0, 22.0, 0.0, 30.8, 0.421, 21.0], [6.0, 104.0, 74.0, 18.0, 156.0, 29.9, 0.722, 41.0], [3.0, 90.0, 78.0, 0.0, 0.0, 42.7, 0.559, 21.0], [3.0, 80.0, 82.0, 31.0, 70.0, 34.2, 1.292, 27.0], [1.0, 119.0, 54.0, 13.0, 50.0, 22.3, 0.205, 24.0], [7.0, 187.0, 68.0, 39.0, 304.0, 37.7, 0.254, 41.0], [7.0, 81.0, 78.0, 40.0, 48.0, 46.7, 0.261, 42.0], [0.0, 118.0, 84.0, 47.0, 230.0, 45.8, 0.551, 31.0], [1.0, 126.0, 60.0, 0.0, 0.0, 30.1, 0.349, 47.0], [10.0, 92.0, 62.0, 0.0, 0.0, 25.9, 0.167, 31.0], [4.0, 99.0, 68.0, 38.0, 0.0, 32.8, 0.145, 33.0], [9.0, 123.0, 70.0, 44.0, 94.0, 33.1, 0.374, 40.0], [2.0, 100.0, 66.0, 20.0, 90.0, 32.9, 0.867, 28.0], [9.0, 152.0, 78.0, 34.0, 171.0, 34.2, 0.893, 33.0], [6.0, 151.0, 62.0, 31.0, 120.0, 35.5, 0.692, 28.0], [0.0, 97.0, 64.0, 36.0, 100.0, 36.8, 0.6, 25.0], [8.0, 85.0, 55.0, 20.0, 0.0, 24.4, 0.136, 42.0], [0.0, 151.0, 90.0, 46.0, 0.0, 42.1, 0.371, 21.0], [2.0, 128.0, 78.0, 37.0, 182.0, 43.3, 1.224, 31.0], [2.0, 121.0, 70.0, 32.0, 95.0, 39.1, 0.886, 23.0], [4.0, 92.0, 80.0, 0.0, 0.0, 42.2, 0.237, 29.0], [5.0, 105.0, 72.0, 29.0, 325.0, 36.9, 0.159, 28.0], [6.0, 99.0, 60.0, 19.0, 54.0, 26.9, 0.497, 32.0], [13.0, 106.0, 72.0, 54.0, 0.0, 36.6, 0.178, 45.0], [12.0, 151.0, 70.0, 40.0, 271.0, 41.8, 0.742, 38.0], [1.0, 90.0, 62.0, 18.0, 59.0, 25.1, 1.268, 25.0], [0.0, 119.0, 64.0, 18.0, 92.0, 34.9, 0.725, 23.0], [1.0, 128.0, 88.0, 39.0, 110.0, 36.5, 1.057, 37.0], [1.0, 120.0, 80.0, 48.0, 200.0, 38.9, 1.162, 41.0], [0.0, 131.0, 66.0, 40.0, 0.0, 34.3, 0.196, 22.0], [6.0, 114.0, 0.0, 0.0, 0.0, 0.0, 0.189, 26.0], [3.0, 113.0, 50.0, 10.0, 85.0, 29.5, 0.626, 25.0], [6.0, 111.0, 64.0, 39.0, 0.0, 34.2, 0.26, 24.0], [5.0, 137.0, 108.0, 0.0, 0.0, 48.8, 0.227, 37.0], [1.0, 128.0, 98.0, 41.0, 58.0, 32.0, 1.321, 33.0], [7.0, 62.0, 78.0, 0.0, 0.0, 32.6, 0.391, 41.0], [1.0, 101.0, 50.0, 15.0, 36.0, 24.2, 0.526, 26.0], [1.0, 135.0, 54.0, 0.0, 0.0, 26.7, 0.687, 62.0], [1.0, 124.0, 60.0, 32.0, 0.0, 35.8, 0.514, 21.0], [1.0, 125.0, 50.0, 40.0, 167.0, 33.3, 0.962, 28.0], [1.0, 82.0, 64.0, 13.0, 95.0, 21.2, 0.415, 23.0], [1.0, 89.0, 76.0, 34.0, 37.0, 31.2, 0.192, 23.0], [8.0, 95.0, 72.0, 0.0, 0.0, 36.8, 0.485, 57.0], [1.0, 89.0, 24.0, 19.0, 25.0, 27.8, 0.559, 21.0], [3.0, 158.0, 64.0, 13.0, 387.0, 31.2, 0.295, 24.0], [1.0, 97.0, 70.0, 15.0, 0.0, 18.2, 0.147, 21.0], [10.0, 122.0, 78.0, 31.0, 0.0, 27.6, 0.512, 45.0], [1.0, 125.0, 70.0, 24.0, 110.0, 24.3, 0.221, 25.0], [1.0, 97.0, 68.0, 21.0, 0.0, 27.2, 1.095, 22.0], [4.0, 94.0, 65.0, 22.0, 0.0, 24.7, 0.148, 21.0], [2.0, 128.0, 64.0, 42.0, 0.0, 40.0, 1.101, 24.0], [1.0, 112.0, 72.0, 30.0, 176.0, 34.4, 0.528, 25.0], [13.0, 126.0, 90.0, 0.0, 0.0, 43.4, 0.583, 42.0], [0.0, 102.0, 52.0, 0.0, 0.0, 25.1, 0.078, 21.0], [1.0, 163.0, 72.0, 0.0, 0.0, 39.0, 1.222, 33.0], [0.0, 131.0, 88.0, 0.0, 0.0, 31.6, 0.743, 32.0], [1.0, 128.0, 82.0, 17.0, 183.0, 27.5, 0.115, 22.0], [3.0, 81.0, 86.0, 16.0, 66.0, 27.5, 0.306, 22.0], [0.0, 139.0, 62.0, 17.0, 210.0, 22.1, 0.207, 21.0], [1.0, 164.0, 82.0, 43.0, 67.0, 32.8, 0.341, 50.0], [3.0, 113.0, 44.0, 13.0, 0.0, 22.4, 0.14, 22.0], [8.0, 194.0, 80.0, 0.0, 0.0, 26.1, 0.551, 67.0], [4.0, 123.0, 62.0, 0.0, 0.0, 32.0, 0.226, 35.0], [2.0, 89.0, 90.0, 30.0, 0.0, 33.5, 0.292, 42.0], [1.0, 139.0, 46.0, 19.0, 83.0, 28.7, 0.654, 22.0], [1.0, 143.0, 84.0, 23.0, 310.0, 42.4, 1.076, 22.0], [4.0, 96.0, 56.0, 17.0, 49.0, 20.8, 0.34, 26.0], [0.0, 109.0, 88.0, 30.0, 0.0, 32.5, 0.855, 38.0], [12.0, 106.0, 80.0, 0.0, 0.0, 23.6, 0.137, 44.0], [0.0, 105.0, 90.0, 0.0, 0.0, 29.6, 0.197, 46.0], [0.0, 114.0, 80.0, 34.0, 285.0, 44.2, 0.167, 27.0], [0.0, 107.0, 62.0, 30.0, 74.0, 36.6, 0.757, 25.0], [6.0, 129.0, 90.0, 7.0, 326.0, 19.6, 0.582, 60.0], [3.0, 61.0, 82.0, 28.0, 0.0, 34.4, 0.243, 46.0], [2.0, 102.0, 86.0, 36.0, 120.0, 45.5, 0.127, 23.0], [2.0, 92.0, 52.0, 0.0, 0.0, 30.1, 0.141, 22.0], [0.0, 167.0, 0.0, 0.0, 0.0, 32.3, 0.839, 30.0], [2.0, 144.0, 58.0, 33.0, 135.0, 31.6, 0.422, 25.0], [8.0, 181.0, 68.0, 36.0, 495.0, 30.1, 0.615, 60.0], [4.0, 129.0, 86.0, 20.0, 270.0, 35.1, 0.231, 23.0], [3.0, 170.0, 64.0, 37.0, 225.0, 34.5, 0.356, 30.0], [2.0, 105.0, 58.0, 40.0, 94.0, 34.9, 0.225, 25.0], [1.0, 108.0, 60.0, 46.0, 178.0, 35.5, 0.415, 24.0], [7.0, 187.0, 50.0, 33.0, 392.0, 33.9, 0.826, 34.0], [2.0, 83.0, 65.0, 28.0, 66.0, 36.8, 0.629, 24.0], [1.0, 116.0, 78.0, 29.0, 180.0, 36.1, 0.496, 25.0], [1.0, 103.0, 30.0, 38.0, 83.0, 43.3, 0.183, 33.0], [5.0, 111.0, 72.0, 28.0, 0.0, 23.9, 0.407, 27.0], [6.0, 92.0, 62.0, 32.0, 126.0, 32.0, 0.085, 46.0], [1.0, 77.0, 56.0, 30.0, 56.0, 33.3, 1.251, 24.0], [5.0, 124.0, 74.0, 0.0, 0.0, 34.0, 0.22, 38.0], [0.0, 100.0, 88.0, 60.0, 110.0, 46.8, 0.962, 31.0], [1.0, 109.0, 38.0, 18.0, 120.0, 23.1, 0.407, 26.0], [4.0, 151.0, 90.0, 38.0, 0.0, 29.7, 0.294, 36.0], [6.0, 165.0, 68.0, 26.0, 168.0, 33.6, 0.631, 49.0], [2.0, 122.0, 76.0, 27.0, 200.0, 35.9, 0.483, 26.0], [5.0, 116.0, 74.0, 29.0, 0.0, 32.3, 0.66, 35.0], [11.0, 155.0, 76.0, 28.0, 150.0, 33.3, 1.353, 51.0], [7.0, 119.0, 0.0, 0.0, 0.0, 25.2, 0.209, 37.0], [0.0, 101.0, 65.0, 28.0, 0.0, 24.6, 0.237, 22.0], [3.0, 142.0, 80.0, 15.0, 0.0, 32.4, 0.2, 63.0], [6.0, 80.0, 66.0, 30.0, 0.0, 26.2, 0.313, 41.0], [2.0, 101.0, 58.0, 35.0, 90.0, 21.8, 0.155, 22.0], [0.0, 124.0, 70.0, 20.0, 0.0, 27.4, 0.254, 36.0], [7.0, 142.0, 90.0, 24.0, 480.0, 30.4, 0.128, 43.0], [4.0, 147.0, 74.0, 25.0, 293.0, 34.9, 0.385, 30.0], [2.0, 85.0, 65.0, 0.0, 0.0, 39.6, 0.93, 27.0], [5.0, 115.0, 98.0, 0.0, 0.0, 52.9, 0.209, 28.0], [1.0, 79.0, 75.0, 30.0, 0.0, 32.0, 0.396, 22.0], [1.0, 119.0, 44.0, 47.0, 63.0, 35.5, 0.28, 25.0], [10.0, 111.0, 70.0, 27.0, 0.0, 27.5, 0.141, 40.0], [5.0, 97.0, 76.0, 27.0, 0.0, 35.6, 0.378, 52.0], [8.0, 188.0, 78.0, 0.0, 0.0, 47.9, 0.137, 43.0], [4.0, 125.0, 70.0, 18.0, 122.0, 28.9, 1.144, 45.0], [6.0, 91.0, 0.0, 0.0, 0.0, 29.8, 0.501, 31.0], [4.0, 183.0, 0.0, 0.0, 0.0, 28.4, 0.212, 36.0], [1.0, 71.0, 62.0, 0.0, 0.0, 21.8, 0.416, 26.0], [2.0, 87.0, 0.0, 23.0, 0.0, 28.9, 0.773, 25.0], [7.0, 133.0, 84.0, 0.0, 0.0, 40.2, 0.696, 37.0], [2.0, 119.0, 0.0, 0.0, 0.0, 19.6, 0.832, 72.0], [4.0, 117.0, 64.0, 27.0, 120.0, 33.2, 0.23, 24.0], [2.0, 83.0, 66.0, 23.0, 50.0, 32.2, 0.497, 22.0], [2.0, 88.0, 58.0, 26.0, 16.0, 28.4, 0.766, 22.0], [0.0, 124.0, 56.0, 13.0, 105.0, 21.8, 0.452, 21.0], [9.0, 120.0, 72.0, 22.0, 56.0, 20.8, 0.733, 48.0], [6.0, 190.0, 92.0, 0.0, 0.0, 35.5, 0.278, 66.0], [11.0, 127.0, 106.0, 0.0, 0.0, 39.0, 0.19, 51.0], [3.0, 120.0, 70.0, 30.0, 135.0, 42.9, 0.452, 30.0], [10.0, 139.0, 80.0, 0.0, 0.0, 27.1, 1.441, 57.0], [4.0, 117.0, 62.0, 12.0, 0.0, 29.7, 0.38, 30.0], [0.0, 137.0, 68.0, 14.0, 148.0, 24.8, 0.143, 21.0], [5.0, 73.0, 60.0, 0.0, 0.0, 26.8, 0.268, 27.0], [4.0, 91.0, 70.0, 32.0, 88.0, 33.1, 0.446, 22.0], [6.0, 166.0, 74.0, 0.0, 0.0, 26.6, 0.304, 66.0], [1.0, 86.0, 66.0, 52.0, 65.0, 41.3, 0.917, 29.0], [0.0, 131.0, 0.0, 0.0, 0.0, 43.2, 0.27, 26.0], [2.0, 108.0, 62.0, 10.0, 278.0, 25.3, 0.881, 22.0], [3.0, 116.0, 0.0, 0.0, 0.0, 23.5, 0.187, 23.0], [1.0, 97.0, 66.0, 15.0, 140.0, 23.2, 0.487, 22.0], [1.0, 124.0, 74.0, 36.0, 0.0, 27.8, 0.1, 30.0], [1.0, 130.0, 60.0, 23.0, 170.0, 28.6, 0.692, 21.0], [9.0, 122.0, 56.0, 0.0, 0.0, 33.3, 1.114, 33.0], [5.0, 126.0, 78.0, 27.0, 22.0, 29.6, 0.439, 40.0], [5.0, 99.0, 54.0, 28.0, 83.0, 34.0, 0.499, 30.0], [5.0, 88.0, 78.0, 30.0, 0.0, 27.6, 0.258, 37.0], [7.0, 114.0, 76.0, 17.0, 110.0, 23.8, 0.466, 31.0], [0.0, 98.0, 82.0, 15.0, 84.0, 25.2, 0.299, 22.0], [3.0, 162.0, 52.0, 38.0, 0.0, 37.2, 0.652, 24.0], [2.0, 75.0, 64.0, 24.0, 55.0, 29.7, 0.37, 33.0], [1.0, 81.0, 72.0, 18.0, 40.0, 26.6, 0.283, 24.0], [7.0, 106.0, 92.0, 18.0, 0.0, 22.7, 0.235, 48.0], [1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0], [0.0, 118.0, 64.0, 23.0, 89.0, 0.0, 1.731, 21.0], [5.0, 130.0, 82.0, 0.0, 0.0, 39.1, 0.956, 37.0], [1.0, 99.0, 58.0, 10.0, 0.0, 25.4, 0.551, 21.0], [2.0, 96.0, 68.0, 13.0, 49.0, 21.1, 0.647, 26.0], [2.0, 87.0, 58.0, 16.0, 52.0, 32.7, 0.166, 25.0], [1.0, 106.0, 76.0, 0.0, 0.0, 37.5, 0.197, 26.0], [9.0, 57.0, 80.0, 37.0, 0.0, 32.8, 0.096, 41.0], [8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0], [1.0, 130.0, 70.0, 13.0, 105.0, 25.9, 0.472, 22.0], [1.0, 147.0, 94.0, 41.0, 0.0, 49.3, 0.358, 27.0], [5.0, 139.0, 80.0, 35.0, 160.0, 31.6, 0.361, 25.0], [1.0, 196.0, 76.0, 36.0, 249.0, 36.5, 0.875, 29.0], [4.0, 85.0, 58.0, 22.0, 49.0, 27.8, 0.306, 28.0], [0.0, 180.0, 90.0, 26.0, 90.0, 36.5, 0.314, 35.0], [6.0, 109.0, 60.0, 27.0, 0.0, 25.0, 0.206, 27.0], [7.0, 178.0, 84.0, 0.0, 0.0, 39.9, 0.331, 41.0], [1.0, 100.0, 74.0, 12.0, 46.0, 19.5, 0.149, 28.0], [2.0, 129.0, 74.0, 26.0, 205.0, 33.2, 0.591, 25.0], [3.0, 173.0, 82.0, 48.0, 465.0, 38.4, 2.137, 25.0], [2.0, 197.0, 70.0, 99.0, 0.0, 34.7, 0.575, 62.0], [1.0, 87.0, 60.0, 37.0, 75.0, 37.2, 0.509, 22.0], [5.0, 139.0, 64.0, 35.0, 140.0, 28.6, 0.411, 26.0], [0.0, 162.0, 76.0, 36.0, 0.0, 49.6, 0.364, 26.0], [6.0, 98.0, 58.0, 33.0, 190.0, 34.0, 0.43, 43.0], [2.0, 123.0, 48.0, 32.0, 165.0, 42.1, 0.52, 26.0], [9.0, 145.0, 88.0, 34.0, 165.0, 30.3, 0.771, 53.0], [1.0, 97.0, 64.0, 19.0, 82.0, 18.2, 0.299, 21.0], [7.0, 83.0, 78.0, 26.0, 71.0, 29.3, 0.767, 36.0], [2.0, 68.0, 62.0, 13.0, 15.0, 20.1, 0.257, 23.0], [4.0, 197.0, 70.0, 39.0, 744.0, 36.7, 2.329, 31.0], [7.0, 94.0, 64.0, 25.0, 79.0, 33.3, 0.738, 41.0], [4.0, 141.0, 74.0, 0.0, 0.0, 27.6, 0.244, 40.0], [2.0, 155.0, 52.0, 27.0, 540.0, 38.7, 0.24, 25.0], [1.0, 126.0, 56.0, 29.0, 152.0, 28.7, 0.801, 21.0], [0.0, 147.0, 85.0, 54.0, 0.0, 42.8, 0.375, 24.0], [5.0, 123.0, 74.0, 40.0, 77.0, 34.1, 0.269, 28.0], [5.0, 117.0, 92.0, 0.0, 0.0, 34.1, 0.337, 38.0], [6.0, 85.0, 78.0, 0.0, 0.0, 31.2, 0.382, 42.0], [0.0, 93.0, 60.0, 25.0, 92.0, 28.7, 0.532, 22.0], [0.0, 126.0, 86.0, 27.0, 120.0, 27.4, 0.515, 21.0], [1.0, 95.0, 66.0, 13.0, 38.0, 19.6, 0.334, 25.0], [6.0, 123.0, 72.0, 45.0, 230.0, 33.6, 0.733, 34.0], [5.0, 158.0, 70.0, 0.0, 0.0, 29.8, 0.207, 63.0], [0.0, 101.0, 64.0, 17.0, 0.0, 21.0, 0.252, 21.0], [17.0, 163.0, 72.0, 41.0, 114.0, 40.9, 0.817, 47.0], [1.0, 71.0, 48.0, 18.0, 76.0, 20.4, 0.323, 22.0], [1.0, 146.0, 56.0, 0.0, 0.0, 29.7, 0.564, 29.0], [3.0, 89.0, 74.0, 16.0, 85.0, 30.4, 0.551, 38.0], [12.0, 88.0, 74.0, 40.0, 54.0, 35.3, 0.378, 48.0], [4.0, 95.0, 70.0, 32.0, 0.0, 32.1, 0.612, 24.0], [7.0, 125.0, 86.0, 0.0, 0.0, 37.6, 0.304, 51.0], [2.0, 141.0, 58.0, 34.0, 128.0, 25.4, 0.699, 24.0], [2.0, 106.0, 56.0, 27.0, 165.0, 29.0, 0.426, 22.0], [3.0, 96.0, 56.0, 34.0, 115.0, 24.7, 0.944, 39.0], [3.0, 102.0, 44.0, 20.0, 94.0, 30.8, 0.4, 26.0], [6.0, 87.0, 80.0, 0.0, 0.0, 23.2, 0.084, 32.0], [7.0, 181.0, 84.0, 21.0, 192.0, 35.9, 0.586, 51.0], [12.0, 140.0, 85.0, 33.0, 0.0, 37.4, 0.244, 41.0], [2.0, 112.0, 68.0, 22.0, 94.0, 34.1, 0.315, 26.0], [0.0, 180.0, 78.0, 63.0, 14.0, 59.4, 2.42, 25.0], [3.0, 141.0, 0.0, 0.0, 0.0, 30.0, 0.761, 27.0], [0.0, 132.0, 78.0, 0.0, 0.0, 32.4, 0.393, 21.0], [3.0, 84.0, 68.0, 30.0, 106.0, 31.9, 0.591, 25.0], [3.0, 102.0, 74.0, 0.0, 0.0, 29.5, 0.121, 32.0], [1.0, 71.0, 78.0, 50.0, 45.0, 33.2, 0.422, 21.0], [13.0, 76.0, 60.0, 0.0, 0.0, 32.8, 0.18, 41.0], [6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0], [0.0, 95.0, 80.0, 45.0, 92.0, 36.5, 0.33, 26.0], [0.0, 146.0, 82.0, 0.0, 0.0, 40.5, 1.781, 44.0], [7.0, 159.0, 66.0, 0.0, 0.0, 30.4, 0.383, 36.0], [1.0, 111.0, 86.0, 19.0, 0.0, 30.1, 0.143, 23.0], [1.0, 83.0, 68.0, 0.0, 0.0, 18.2, 0.624, 27.0], [2.0, 112.0, 78.0, 50.0, 140.0, 39.4, 0.175, 24.0], [0.0, 188.0, 82.0, 14.0, 185.0, 32.0, 0.682, 22.0], [3.0, 128.0, 78.0, 0.0, 0.0, 21.1, 0.268, 55.0], [5.0, 121.0, 72.0, 23.0, 112.0, 26.2, 0.245, 30.0], [4.0, 154.0, 62.0, 31.0, 284.0, 32.8, 0.237, 23.0], [4.0, 189.0, 110.0, 31.0, 0.0, 28.5, 0.68, 37.0], [8.0, 107.0, 80.0, 0.0, 0.0, 24.6, 0.856, 34.0], [5.0, 88.0, 66.0, 21.0, 23.0, 24.4, 0.342, 30.0], [2.0, 68.0, 70.0, 32.0, 66.0, 25.0, 0.187, 25.0], [1.0, 153.0, 82.0, 42.0, 485.0, 40.6, 0.687, 23.0], [14.0, 100.0, 78.0, 25.0, 184.0, 36.6, 0.412, 46.0], [3.0, 150.0, 76.0, 0.0, 0.0, 21.0, 0.207, 37.0], [0.0, 106.0, 70.0, 37.0, 148.0, 39.4, 0.605, 22.0], [7.0, 114.0, 66.0, 0.0, 0.0, 32.8, 0.258, 42.0], [3.0, 124.0, 80.0, 33.0, 130.0, 33.2, 0.305, 26.0], [6.0, 124.0, 72.0, 0.0, 0.0, 27.6, 0.368, 29.0], [3.0, 132.0, 80.0, 0.0, 0.0, 34.4, 0.402, 44.0], [8.0, 133.0, 72.0, 0.0, 0.0, 32.9, 0.27, 39.0], [1.0, 95.0, 60.0, 18.0, 58.0, 23.9, 0.26, 22.0], [3.0, 125.0, 58.0, 0.0, 0.0, 31.6, 0.151, 24.0], [6.0, 137.0, 61.0, 0.0, 0.0, 24.2, 0.151, 55.0], [6.0, 134.0, 80.0, 37.0, 370.0, 46.2, 0.238, 46.0], [9.0, 72.0, 78.0, 25.0, 0.0, 31.6, 0.28, 38.0], [1.0, 107.0, 72.0, 30.0, 82.0, 30.8, 0.821, 24.0], [1.0, 100.0, 72.0, 12.0, 70.0, 25.3, 0.658, 28.0], [5.0, 144.0, 82.0, 26.0, 285.0, 32.0, 0.452, 58.0], [2.0, 127.0, 46.0, 21.0, 335.0, 34.4, 0.176, 22.0], [2.0, 90.0, 80.0, 14.0, 55.0, 24.4, 0.249, 24.0], [12.0, 140.0, 82.0, 43.0, 325.0, 39.2, 0.528, 58.0], [10.0, 161.0, 68.0, 23.0, 132.0, 25.5, 0.326, 47.0], [5.0, 147.0, 75.0, 0.0, 0.0, 29.9, 0.434, 28.0], [0.0, 104.0, 64.0, 37.0, 64.0, 33.6, 0.51, 22.0], [10.0, 108.0, 66.0, 0.0, 0.0, 32.4, 0.272, 42.0], [2.0, 111.0, 60.0, 0.0, 0.0, 26.2, 0.343, 23.0], [1.0, 80.0, 74.0, 11.0, 60.0, 30.0, 0.527, 22.0], [8.0, 109.0, 76.0, 39.0, 114.0, 27.9, 0.64, 31.0], [1.0, 118.0, 58.0, 36.0, 94.0, 33.3, 0.261, 23.0], [6.0, 105.0, 70.0, 32.0, 68.0, 30.8, 0.122, 37.0], [5.0, 132.0, 80.0, 0.0, 0.0, 26.8, 0.186, 69.0], [5.0, 106.0, 82.0, 30.0, 0.0, 39.5, 0.286, 38.0], [1.0, 173.0, 74.0, 0.0, 0.0, 36.8, 0.088, 38.0], [3.0, 111.0, 56.0, 39.0, 0.0, 30.1, 0.557, 30.0], [1.0, 181.0, 64.0, 30.0, 180.0, 34.1, 0.328, 38.0], [2.0, 101.0, 58.0, 17.0, 265.0, 24.2, 0.614, 23.0], [1.0, 93.0, 70.0, 31.0, 0.0, 30.4, 0.315, 23.0], [0.0, 74.0, 52.0, 10.0, 36.0, 27.8, 0.269, 22.0], [1.0, 79.0, 80.0, 25.0, 37.0, 25.4, 0.583, 22.0], [1.0, 0.0, 74.0, 20.0, 23.0, 27.7, 0.299, 21.0], [1.0, 149.0, 68.0, 29.0, 127.0, 29.3, 0.349, 42.0], [1.0, 140.0, 74.0, 26.0, 180.0, 24.1, 0.828, 23.0], [2.0, 108.0, 64.0, 0.0, 0.0, 30.8, 0.158, 21.0], [0.0, 117.0, 66.0, 31.0, 188.0, 30.8, 0.493, 22.0], [6.0, 154.0, 78.0, 41.0, 140.0, 46.1, 0.571, 27.0], [1.0, 128.0, 48.0, 45.0, 194.0, 40.5, 0.613, 24.0], [4.0, 76.0, 62.0, 0.0, 0.0, 34.0, 0.391, 25.0], [0.0, 101.0, 62.0, 0.0, 0.0, 21.9, 0.336, 25.0], [4.0, 112.0, 78.0, 40.0, 0.0, 39.4, 0.236, 38.0], [6.0, 125.0, 68.0, 30.0, 120.0, 30.0, 0.464, 32.0], [2.0, 129.0, 84.0, 0.0, 0.0, 28.0, 0.284, 27.0], [1.0, 95.0, 82.0, 25.0, 180.0, 35.0, 0.233, 43.0], [1.0, 180.0, 0.0, 0.0, 0.0, 43.3, 0.282, 41.0], [7.0, 137.0, 90.0, 41.0, 0.0, 32.0, 0.391, 39.0], [6.0, 162.0, 62.0, 0.0, 0.0, 24.3, 0.178, 50.0], [2.0, 106.0, 64.0, 35.0, 119.0, 30.5, 1.4, 34.0], [0.0, 123.0, 72.0, 0.0, 0.0, 36.3, 0.258, 52.0], [10.0, 129.0, 62.0, 36.0, 0.0, 41.2, 0.441, 38.0], [8.0, 108.0, 70.0, 0.0, 0.0, 30.5, 0.955, 33.0], [2.0, 127.0, 58.0, 24.0, 275.0, 27.7, 1.6, 25.0], [2.0, 92.0, 76.0, 20.0, 0.0, 24.2, 1.698, 28.0], [4.0, 128.0, 70.0, 0.0, 0.0, 34.3, 0.303, 24.0], [0.0, 145.0, 0.0, 0.0, 0.0, 44.2, 0.63, 31.0], [4.0, 145.0, 82.0, 18.0, 0.0, 32.5, 0.235, 70.0], [6.0, 183.0, 94.0, 0.0, 0.0, 40.8, 1.461, 45.0], [3.0, 78.0, 70.0, 0.0, 0.0, 32.5, 0.27, 39.0], [8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0], [6.0, 115.0, 60.0, 39.0, 0.0, 33.7, 0.245, 40.0], [5.0, 86.0, 68.0, 28.0, 71.0, 30.2, 0.364, 24.0], [1.0, 119.0, 88.0, 41.0, 170.0, 45.3, 0.507, 26.0], [1.0, 133.0, 102.0, 28.0, 140.0, 32.8, 0.234, 45.0], [0.0, 134.0, 58.0, 20.0, 291.0, 26.4, 0.352, 21.0], [12.0, 84.0, 72.0, 31.0, 0.0, 29.7, 0.297, 46.0], [0.0, 146.0, 70.0, 0.0, 0.0, 37.9, 0.334, 28.0], [4.0, 137.0, 84.0, 0.0, 0.0, 31.2, 0.252, 30.0], [1.0, 131.0, 64.0, 14.0, 415.0, 23.7, 0.389, 21.0], [7.0, 152.0, 88.0, 44.0, 0.0, 50.0, 0.337, 36.0], [3.0, 122.0, 78.0, 0.0, 0.0, 23.0, 0.254, 40.0], [1.0, 91.0, 54.0, 25.0, 100.0, 25.2, 0.234, 23.0], [0.0, 111.0, 65.0, 0.0, 0.0, 24.6, 0.66, 31.0], [6.0, 147.0, 80.0, 0.0, 0.0, 29.5, 0.178, 50.0], [7.0, 179.0, 95.0, 31.0, 0.0, 34.2, 0.164, 60.0], [1.0, 193.0, 50.0, 16.0, 375.0, 25.9, 0.655, 24.0], [8.0, 197.0, 74.0, 0.0, 0.0, 25.9, 1.191, 39.0], [0.0, 102.0, 86.0, 17.0, 105.0, 29.3, 0.695, 27.0], [6.0, 125.0, 76.0, 0.0, 0.0, 33.8, 0.121, 54.0], [11.0, 138.0, 74.0, 26.0, 144.0, 36.1, 0.557, 50.0], [3.0, 112.0, 74.0, 30.0, 0.0, 31.6, 0.197, 25.0], [0.0, 95.0, 85.0, 25.0, 36.0, 37.4, 0.247, 24.0], [0.0, 129.0, 110.0, 46.0, 130.0, 67.1, 0.319, 26.0], [9.0, 165.0, 88.0, 0.0, 0.0, 30.4, 0.302, 49.0], [1.0, 90.0, 68.0, 8.0, 0.0, 24.5, 1.138, 36.0], [4.0, 99.0, 72.0, 17.0, 0.0, 25.6, 0.294, 28.0], [3.0, 106.0, 54.0, 21.0, 158.0, 30.9, 0.292, 24.0], [10.0, 125.0, 70.0, 26.0, 115.0, 31.1, 0.205, 41.0], [0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0], [0.0, 135.0, 94.0, 46.0, 145.0, 40.6, 0.284, 26.0], [9.0, 134.0, 74.0, 33.0, 60.0, 25.9, 0.46, 81.0], [9.0, 140.0, 94.0, 0.0, 0.0, 32.7, 0.734, 45.0], [8.0, 176.0, 90.0, 34.0, 300.0, 33.7, 0.467, 58.0], [1.0, 109.0, 60.0, 8.0, 182.0, 25.4, 0.947, 21.0], [3.0, 111.0, 58.0, 31.0, 44.0, 29.5, 0.43, 22.0], [1.0, 117.0, 88.0, 24.0, 145.0, 34.5, 0.403, 40.0], [4.0, 95.0, 60.0, 32.0, 0.0, 35.4, 0.284, 28.0], [1.0, 121.0, 78.0, 39.0, 74.0, 39.0, 0.261, 28.0], [1.0, 84.0, 64.0, 23.0, 115.0, 36.9, 0.471, 28.0], [0.0, 137.0, 70.0, 38.0, 0.0, 33.2, 0.17, 22.0], [7.0, 196.0, 90.0, 0.0, 0.0, 39.8, 0.451, 41.0], [5.0, 99.0, 74.0, 27.0, 0.0, 29.0, 0.203, 32.0], [5.0, 122.0, 86.0, 0.0, 0.0, 34.7, 0.29, 33.0], [5.0, 0.0, 80.0, 32.0, 0.0, 41.0, 0.346, 37.0], [0.0, 141.0, 84.0, 26.0, 0.0, 32.4, 0.433, 22.0], [2.0, 100.0, 54.0, 28.0, 105.0, 37.8, 0.498, 24.0], [9.0, 164.0, 84.0, 21.0, 0.0, 30.8, 0.831, 32.0], [11.0, 138.0, 76.0, 0.0, 0.0, 33.2, 0.42, 35.0], [10.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.261, 30.0], [1.0, 108.0, 88.0, 19.0, 0.0, 27.1, 0.4, 24.0], [3.0, 111.0, 90.0, 12.0, 78.0, 28.4, 0.495, 29.0], [0.0, 128.0, 68.0, 19.0, 180.0, 30.5, 1.391, 25.0], [6.0, 0.0, 68.0, 41.0, 0.0, 39.0, 0.727, 41.0], [9.0, 91.0, 68.0, 0.0, 0.0, 24.2, 0.2, 58.0], [7.0, 133.0, 88.0, 15.0, 155.0, 32.4, 0.262, 37.0], [1.0, 168.0, 88.0, 29.0, 0.0, 35.0, 0.905, 52.0], [1.0, 105.0, 58.0, 0.0, 0.0, 24.3, 0.187, 21.0], [3.0, 148.0, 66.0, 25.0, 0.0, 32.5, 0.256, 22.0], [2.0, 99.0, 0.0, 0.0, 0.0, 22.2, 0.108, 23.0], [7.0, 150.0, 66.0, 42.0, 342.0, 34.7, 0.718, 42.0], [2.0, 122.0, 52.0, 43.0, 158.0, 36.2, 0.816, 28.0], [11.0, 120.0, 80.0, 37.0, 150.0, 42.3, 0.785, 48.0], [6.0, 80.0, 80.0, 36.0, 0.0, 39.8, 0.177, 28.0], [8.0, 84.0, 74.0, 31.0, 0.0, 38.3, 0.457, 39.0], [5.0, 110.0, 68.0, 0.0, 0.0, 26.0, 0.292, 30.0], [13.0, 158.0, 114.0, 0.0, 0.0, 42.3, 0.257, 44.0], [4.0, 122.0, 68.0, 0.0, 0.0, 35.0, 0.394, 29.0], [1.0, 199.0, 76.0, 43.0, 0.0, 42.9, 1.394, 22.0]]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gime-Me-Some-Credit *"
      ],
      "metadata": {
        "id": "p4jrTXtPhar8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/credit_test.csv\"\n",
        "X_credit_train, y_credit_train = kagData(path)\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_credit_train)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename\n",
        "Credit_X_train = X_credit_train\n",
        "Credit_y_train = y_label"
      ],
      "metadata": {
        "id": "8bXOybiOhasD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/credit_training.csv\"\n",
        "X_credit_test, y_credit_test = kagData(path)\n"
      ],
      "metadata": {
        "id": "raLBUGHPhweZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Poker Hand"
      ],
      "metadata": {
        "id": "ykNICZ4Ji42d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/poker-hand-training-true.data\"\n",
        "names = [\"S1\", \"C1\", \"S2\", \"C2\",\"S3\", \"C3\", \"S4\", \"C4\",\"S5\", \"C5\", \"Class\"]\n",
        "X_poker_train, y_poker_train = uciData(path, names)"
      ],
      "metadata": {
        "id": "UYy8Ztdhi42k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/poker-hand-testing.data\"\n",
        "names = [\"S1\", \"C1\", \"S2\", \"C2\",\"S3\", \"C3\", \"S4\", \"C4\",\"S5\", \"C5\", \"Class\"]\n",
        "X_poker_test, y__poker_test = uciData(path, names)"
      ],
      "metadata": {
        "id": "i2c28HyPjJbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### German Credit Data"
      ],
      "metadata": {
        "id": "l1HQkoi6ktzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_german = statlog_german_credit_data.data.features\n",
        "y_german = statlog_german_credit_data.data.targets\n",
        "\n",
        "# metadata\n",
        "print(statlog_german_credit_data.metadata)\n",
        "\n",
        "# variable information\n",
        "print(statlog_german_credit_data.variables)\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_german)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "German_X_train, German_X_test, German_y_train, German_y_test = train_test_split(X_german, y_label, test_size=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5136kW2dktz0",
        "outputId": "ff5328be-d8bf-48b8-cb10-c78ff5de9215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 144, 'name': 'Statlog (German Credit Data)', 'repository_url': 'https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data', 'data_url': 'https://archive.ics.uci.edu/static/public/144/data.csv', 'abstract': 'This dataset classifies people described by a set of attributes as good or bad credit risks. Comes in two formats (one all numeric). Also comes with a cost matrix', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1000, 'num_features': 20, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Other', 'Marital Status', 'Age', 'Occupation'], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1994, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5NC77', 'creators': ['Hans Hofmann'], 'intro_paper': None, 'additional_info': {'summary': 'Two datasets are provided.  the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file \"german.data\".   \\r\\n \\r\\nFor algorithms that need numerical attributes, Strathclyde University produced the file \"german.data-numeric\".  This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables.   Several attributes that are ordered categorical (such as attribute 17) have been coded as integer.    This was the form used by StatLog.\\r\\n\\r\\nThis dataset requires use of a cost matrix (see below)\\r\\n\\r\\n ..... 1        2\\r\\n----------------------------\\r\\n  1   0        1\\r\\n-----------------------\\r\\n  2   5        0\\r\\n\\r\\n(1 = Good,  2 = Bad)\\r\\n\\r\\nThe rows represent the actual classification and the columns the predicted classification.\\r\\n\\r\\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Attribute 1:  (qualitative)      \\r\\n Status of existing checking account\\r\\n             A11 :      ... <    0 DM\\r\\n\\t       A12 : 0 <= ... <  200 DM\\r\\n\\t       A13 :      ... >= 200 DM / salary assignments for at least 1 year\\r\\n               A14 : no checking account\\r\\n\\r\\nAttribute 2:  (numerical)\\r\\n\\t      Duration in month\\r\\n\\r\\nAttribute 3:  (qualitative)\\r\\n\\t      Credit history\\r\\n\\t      A30 : no credits taken/ all credits paid back duly\\r\\n              A31 : all credits at this bank paid back duly\\r\\n\\t      A32 : existing credits paid back duly till now\\r\\n              A33 : delay in paying off in the past\\r\\n\\t      A34 : critical account/  other credits existing (not at this bank)\\r\\n\\r\\nAttribute 4:  (qualitative)\\r\\n\\t      Purpose\\r\\n\\t      A40 : car (new)\\r\\n\\t      A41 : car (used)\\r\\n\\t      A42 : furniture/equipment\\r\\n\\t      A43 : radio/television\\r\\n\\t      A44 : domestic appliances\\r\\n\\t      A45 : repairs\\r\\n\\t      A46 : education\\r\\n\\t      A47 : (vacation - does not exist?)\\r\\n\\t      A48 : retraining\\r\\n\\t      A49 : business\\r\\n\\t      A410 : others\\r\\n\\r\\nAttribute 5:  (numerical)\\r\\n\\t      Credit amount\\r\\n\\r\\nAttibute 6:  (qualitative)\\r\\n\\t      Savings account/bonds\\r\\n\\t      A61 :          ... <  100 DM\\r\\n\\t      A62 :   100 <= ... <  500 DM\\r\\n\\t      A63 :   500 <= ... < 1000 DM\\r\\n\\t      A64 :          .. >= 1000 DM\\r\\n              A65 :   unknown/ no savings account\\r\\n\\r\\nAttribute 7:  (qualitative)\\r\\n\\t      Present employment since\\r\\n\\t      A71 : unemployed\\r\\n\\t      A72 :       ... < 1 year\\r\\n\\t      A73 : 1  <= ... < 4 years  \\r\\n\\t      A74 : 4  <= ... < 7 years\\r\\n\\t      A75 :       .. >= 7 years\\r\\n\\r\\nAttribute 8:  (numerical)\\r\\n\\t      Installment rate in percentage of disposable income\\r\\n\\r\\nAttribute 9:  (qualitative)\\r\\n\\t      Personal status and sex\\r\\n\\t      A91 : male   : divorced/separated\\r\\n\\t      A92 : female : divorced/separated/married\\r\\n              A93 : male   : single\\r\\n\\t      A94 : male   : married/widowed\\r\\n\\t      A95 : female : single\\r\\n\\r\\nAttribute 10: (qualitative)\\r\\n\\t      Other debtors / guarantors\\r\\n\\t      A101 : none\\r\\n\\t      A102 : co-applicant\\r\\n\\t      A103 : guarantor\\r\\n\\r\\nAttribute 11: (numerical)\\r\\n\\t      Present residence since\\r\\n\\r\\nAttribute 12: (qualitative)\\r\\n\\t      Property\\r\\n\\t      A121 : real estate\\r\\n\\t      A122 : if not A121 : building society savings agreement/ life insurance\\r\\n              A123 : if not A121/A122 : car or other, not in attribute 6\\r\\n\\t      A124 : unknown / no property\\r\\n\\r\\nAttribute 13: (numerical)\\r\\n\\t      Age in years\\r\\n\\r\\nAttribute 14: (qualitative)\\r\\n\\t      Other installment plans \\r\\n\\t      A141 : bank\\r\\n\\t      A142 : stores\\r\\n\\t      A143 : none\\r\\n\\r\\nAttribute 15: (qualitative)\\r\\n\\t      Housing\\r\\n\\t      A151 : rent\\r\\n\\t      A152 : own\\r\\n\\t      A153 : for free\\r\\n\\r\\nAttribute 16: (numerical)\\r\\n              Number of existing credits at this bank\\r\\n\\r\\nAttribute 17: (qualitative)\\r\\n\\t      Job\\r\\n\\t      A171 : unemployed/ unskilled  - non-resident\\r\\n\\t      A172 : unskilled - resident\\r\\n\\t      A173 : skilled employee / official\\r\\n\\t      A174 : management/ self-employed/\\r\\n\\t\\t     highly qualified employee/ officer\\r\\n\\r\\nAttribute 18: (numerical)\\r\\n\\t      Number of people being liable to provide maintenance for\\r\\n\\r\\nAttribute 19: (qualitative)\\r\\n\\t      Telephone\\r\\n\\t      A191 : none\\r\\n\\t      A192 : yes, registered under the customers name\\r\\n\\r\\nAttribute 20: (qualitative)\\r\\n\\t      foreign worker\\r\\n\\t      A201 : yes\\r\\n\\t      A202 : no\\r\\n', 'citation': None}}\n",
            "           name     role         type     demographic  \\\n",
            "0    Attribute1  Feature  Categorical            None   \n",
            "1    Attribute2  Feature      Integer            None   \n",
            "2    Attribute3  Feature  Categorical            None   \n",
            "3    Attribute4  Feature  Categorical            None   \n",
            "4    Attribute5  Feature      Integer            None   \n",
            "5    Attribute6  Feature  Categorical            None   \n",
            "6    Attribute7  Feature  Categorical           Other   \n",
            "7    Attribute8  Feature      Integer            None   \n",
            "8    Attribute9  Feature  Categorical  Marital Status   \n",
            "9   Attribute10  Feature  Categorical            None   \n",
            "10  Attribute11  Feature      Integer            None   \n",
            "11  Attribute12  Feature  Categorical            None   \n",
            "12  Attribute13  Feature      Integer             Age   \n",
            "13  Attribute14  Feature  Categorical            None   \n",
            "14  Attribute15  Feature  Categorical           Other   \n",
            "15  Attribute16  Feature      Integer            None   \n",
            "16  Attribute17  Feature  Categorical      Occupation   \n",
            "17  Attribute18  Feature      Integer            None   \n",
            "18  Attribute19  Feature       Binary            None   \n",
            "19  Attribute20  Feature       Binary           Other   \n",
            "20        class   Target       Binary            None   \n",
            "\n",
            "                                          description   units missing_values  \n",
            "0                 Status of existing checking account    None             no  \n",
            "1                                            Duration  months             no  \n",
            "2                                      Credit history    None             no  \n",
            "3                                             Purpose    None             no  \n",
            "4                                       Credit amount    None             no  \n",
            "5                               Savings account/bonds    None             no  \n",
            "6                            Present employment since    None             no  \n",
            "7   Installment rate in percentage of disposable i...    None             no  \n",
            "8                             Personal status and sex    None             no  \n",
            "9                          Other debtors / guarantors    None             no  \n",
            "10                            Present residence since    None             no  \n",
            "11                                           Property    None             no  \n",
            "12                                                Age   years             no  \n",
            "13                            Other installment plans    None             no  \n",
            "14                                            Housing    None             no  \n",
            "15            Number of existing credits at this bank    None             no  \n",
            "16                                                Job    None             no  \n",
            "17  Number of people being liable to provide maint...    None             no  \n",
            "18                                          Telephone    None             no  \n",
            "19                                     foreign worker    None             no  \n",
            "20                                  1 = Good, 2 = Bad    None             no  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Segmentation"
      ],
      "metadata": {
        "id": "JzfEriXulAXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "image_segmentation = fetch_ucirepo(id=50)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_image = image_segmentation.data.features\n",
        "y_image = image_segmentation.data.targets\n",
        "\n",
        "# metadata\n",
        "print(image_segmentation.metadata)\n",
        "\n",
        "# variable information\n",
        "print(image_segmentation.variables)\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_image)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "Image_X_train, Image_X_test, Image_y_train, Image_y_test = train_test_split(X_image, y_label, test_size=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Bv7CghlAXf",
        "outputId": "ed13274a-54fd-49b2-b04f-5e7411812079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 50, 'name': 'Image Segmentation', 'repository_url': 'https://archive.ics.uci.edu/dataset/50/image+segmentation', 'data_url': 'https://archive.ics.uci.edu/static/public/50/data.csv', 'abstract': 'Image data described by high-level numeric-valued attributes, 7 classes', 'area': 'Other', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 2310, 'num_features': 19, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1990, 'last_updated': 'Fri Oct 27 2023', 'dataset_doi': '10.24432/C5GP4N', 'creators': [], 'intro_paper': None, 'additional_info': {'summary': 'The instances were drawn randomly from a database of 7 outdoor images.  The images were handsegmented to create a classification for every pixel.  \\r\\n\\r\\n   Each instance is a 3x3 region.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '    1.  region-centroid-col:  the column of the center pixel of the region.\\r\\n    2.  region-centroid-row:  the row of the center pixel of the region.\\r\\n    3.  region-pixel-count:  the number of pixels in a region = 9.\\r\\n    4.  short-line-density-5:  the results of a line extractoin algorithm that counts how many lines of length 5 (any orientation) with low contrast, less than or equal to 5, go through the region.\\r\\n    5.  short-line-density-2:  same as short-line-density-5 but counts lines of high contrast, greater than 5.\\r\\n    6.  vedge-mean:  measure the contrast of horizontally adjacent pixels in the region.  There are 6, the mean and standard deviation are given.  This attribute is used as a vertical edge detector.\\r\\n    7.  vegde-sd:  (see 6)\\r\\n    8.  hedge-mean:  measures the contrast of vertically adjacent pixels. Used for horizontal line detection. \\r\\n    9.  hedge-sd: (see 8).\\r\\n    10. intensity-mean:  the average over the region of (R + G + B)/3\\r\\n    11. rawred-mean: the average over the region of the R value.\\r\\n    12. rawblue-mean: the average over the region of the B value.\\r\\n    13. rawgreen-mean: the average over the region of the G value.\\r\\n    14. exred-mean: measure the excess red:  (2R - (G + B))\\r\\n    15. exblue-mean: measure the excess blue:  (2B - (G + R))\\r\\n    16. exgreen-mean: measure the excess green:  (2G - (R + B))\\r\\n    17. value-mean:  3-d nonlinear transformation of RGB. (Algorithm can be found in Foley and VanDam, Fundamentals of Interactive Computer Graphics)\\r\\n    18. saturatoin-mean:  (see 17)\\r\\n    19. hue-mean:  (see 17)', 'citation': None}}\n",
            "                    name     role         type demographic  \\\n",
            "0                  class   Target  Categorical        None   \n",
            "1    region-centroid-col  Feature   Continuous        None   \n",
            "2    region-centroid-row  Feature   Continuous        None   \n",
            "3     region-pixel-count  Feature   Continuous        None   \n",
            "4   short-line-density-5  Feature   Continuous        None   \n",
            "5   short-line-density-2  Feature   Continuous        None   \n",
            "6             vedge-mean  Feature   Continuous        None   \n",
            "7               vedge-sd  Feature   Continuous        None   \n",
            "8             hedge-mean  Feature   Continuous        None   \n",
            "9               hedge-sd  Feature   Continuous        None   \n",
            "10        intensity-mean  Feature   Continuous        None   \n",
            "11           rawred-mean  Feature   Continuous        None   \n",
            "12          rawblue-mean  Feature   Continuous        None   \n",
            "13         rawgreen-mean  Feature   Continuous        None   \n",
            "14            exred-mean  Feature   Continuous        None   \n",
            "15           exblue-mean  Feature   Continuous        None   \n",
            "16          exgreen-mean  Feature   Continuous        None   \n",
            "17            value-mean  Feature   Continuous        None   \n",
            "18       saturation-mean  Feature   Continuous        None   \n",
            "19              hue-mean  Feature   Continuous        None   \n",
            "\n",
            "                                          description units missing_values  \n",
            "0                                                None  None             no  \n",
            "1        the column of the center pixel of the region  None             no  \n",
            "2           the row of the center pixel of the region  None             no  \n",
            "3                the number of pixels in a region = 9  None             no  \n",
            "4   the results of a line extractoin algorithm tha...  None             no  \n",
            "5   same as short-line-density-5 but counts lines ...  None             no  \n",
            "6   measure the contrast of horizontally adjacent ...  None             no  \n",
            "7                                               see 6  None             no  \n",
            "8   measures the contrast of vertically adjacent p...  None             no  \n",
            "9                                               see 8  None             no  \n",
            "10       the average over the region of (R + G + B)/3  None             no  \n",
            "11        the average over the region of the R value.  None             no  \n",
            "12        the average over the region of the B value.  None             no  \n",
            "13        the average over the region of the G value.  None             no  \n",
            "14            measure the excess red:  (2R - (G + B))  None             no  \n",
            "15           measure the excess blue:  (2B - (G + R))  None             no  \n",
            "16          measure the excess green:  (2G - (R + B))  None             no  \n",
            "17  3-d nonlinear transformation of RGB. (Algorith...  None             no  \n",
            "18                                             see 17  None             no  \n",
            "19                                             see 17  None             no  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Covertype"
      ],
      "metadata": {
        "id": "fEUs-Y87lRS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "covertype = fetch_ucirepo(id=31)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_covertype = covertype.data.features\n",
        "y_covertype = covertype.data.targets\n",
        "print(y_covertype)\n",
        "print(X_covertype)\n",
        "# metadata\n",
        "print(covertype.metadata)\n",
        "\n",
        "# variable information\n",
        "print(covertype.variables)\n",
        "\n",
        "#Onehot encoding\n",
        "y_enc = LabelEncoder().fit_transform(y_covertype)\n",
        "# Converting the label into a matrix form\n",
        "y_label = tf.keras.utils.to_categorical(y_enc)\n",
        "\n",
        "#Rename and split into train and test\n",
        "Cover_X_train, Cover_X_test, Cover_y_train, Cover_y_test = train_test_split(X_covertype, y_label, test_size=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dRe8XamClRS-",
        "outputId": "0188bc66-fca4-4f09-f8e0-d4aead297413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "             Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
            "2596 51  3         258       0    510                               221   \n",
            "2590 56  2         212      -6    390                               220   \n",
            "2804 139 9         268      65   3180                               234   \n",
            "2785 155 18        242     118   3090                               238   \n",
            "2595 45  2         153      -1    391                               220   \n",
            "...                ...     ...    ...                               ...   \n",
            "2396 153 20         85      17    108                               240   \n",
            "2391 152 19         67      12     95                               240   \n",
            "2386 159 17         60       7     90                               236   \n",
            "2384 170 15         60       5     90                               230   \n",
            "2383 165 13         60       4     67                               231   \n",
            "\n",
            "             Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
            "2596 51  3                              232                              148   \n",
            "2590 56  2                              235                              151   \n",
            "2804 139 9                              238                              135   \n",
            "2785 155 18                             238                              122   \n",
            "2595 45  2                              234                              150   \n",
            "...                                     ...                              ...   \n",
            "2396 153 20                             237                              118   \n",
            "2391 152 19                             237                              119   \n",
            "2386 159 17                             241                              130   \n",
            "2384 170 15                             245                              143   \n",
            "2383 165 13                             244                              141   \n",
            "\n",
            "             Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
            "2596 51  3            6279               1              0   \n",
            "2590 56  2            6225               1              0   \n",
            "2804 139 9            6121               1              0   \n",
            "2785 155 18           6211               1              0   \n",
            "2595 45  2            6172               1              0   \n",
            "...                    ...             ...            ...   \n",
            "2396 153 20            837               0              0   \n",
            "2391 152 19            845               0              0   \n",
            "2386 159 17            854               0              0   \n",
            "2384 170 15            864               0              0   \n",
            "2383 165 13            875               0              0   \n",
            "\n",
            "             Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  \\\n",
            "2596 51  3                                    0  ...            0   \n",
            "2590 56  2                                    0  ...            0   \n",
            "2804 139 9                                    0  ...            0   \n",
            "2785 155 18                                   0  ...            0   \n",
            "2595 45  2                                    0  ...            0   \n",
            "...                                         ...  ...          ...   \n",
            "2396 153 20                                   1  ...            0   \n",
            "2391 152 19                                   1  ...            0   \n",
            "2386 159 17                                   1  ...            0   \n",
            "2384 170 15                                   1  ...            0   \n",
            "2383 165 13                                   1  ...            0   \n",
            "\n",
            "             Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
            "2596 51  3             0            0            0            0            0   \n",
            "2590 56  2             0            0            0            0            0   \n",
            "2804 139 9             0            0            0            0            0   \n",
            "2785 155 18            0            0            0            0            0   \n",
            "2595 45  2             0            0            0            0            0   \n",
            "...                  ...          ...          ...          ...          ...   \n",
            "2396 153 20            0            0            0            0            0   \n",
            "2391 152 19            0            0            0            0            0   \n",
            "2386 159 17            0            0            0            0            0   \n",
            "2384 170 15            0            0            0            0            0   \n",
            "2383 165 13            0            0            0            0            0   \n",
            "\n",
            "             Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
            "2596 51  3             0            0            0           5  \n",
            "2590 56  2             0            0            0           5  \n",
            "2804 139 9             0            0            0           2  \n",
            "2785 155 18            0            0            0           2  \n",
            "2595 45  2             0            0            0           5  \n",
            "...                  ...          ...          ...         ...  \n",
            "2396 153 20            0            0            0           3  \n",
            "2391 152 19            0            0            0           3  \n",
            "2386 159 17            0            0            0           3  \n",
            "2384 170 15            0            0            0           3  \n",
            "2383 165 13            0            0            0           3  \n",
            "\n",
            "[581012 rows x 52 columns]\n",
            "{'uci_id': 31, 'name': 'Covertype', 'repository_url': 'https://archive.ics.uci.edu/dataset/31/covertype', 'data_url': 'https://archive.ics.uci.edu/static/public/31/data.csv', 'abstract': 'Classification of pixels into 7 forest cover types based on attributes such as elevation, aspect, slope, hillshade, soil-type, and more.', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 581012, 'num_features': 52, 'feature_types': ['Categorical', 'Integer'], 'demographics': [], 'target_col': None, 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1998, 'last_updated': 'Fri Oct 27 2023', 'dataset_doi': '10.24432/C50K5N', 'creators': ['Jock Blackard'], 'intro_paper': None, 'additional_info': {'summary': 'Predicting forest cover type from cartographic variables only (no remotely sensed data).  The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data.  Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data.  Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).\\r\\n\\r\\nThis study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado.  These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\\r\\n\\r\\nSome background information for these four wilderness areas: Neota (area 2) probably has the highest mean elevational value of the 4 wilderness areas. Rawah (area 1) and Comanche Peak (area 3) would have a lower mean elevational value, while Cache la Poudre (area 4) would have the lowest mean elevational value. \\r\\n\\r\\nAs for primary major tree species in these areas, Neota would have spruce/fir (type 1), while Rawah and Comanche Peak would probably have lodgepole pine (type 2) as their primary species, followed by spruce/fir and aspen (type 5). Cache la Poudre would tend to have Ponderosa pine (type 3), Douglas-fir (type 6), and cottonwood/willow (type 4).  \\r\\n\\r\\nThe Rawah and Comanche Peak areas would tend to be more typical of the overall dataset than either the Neota or Cache la Poudre, due to their assortment of tree species and range of predictive variable values (elevation, etc.)  Cache la Poudre would probably  be more unique than the others, due to its relatively low  elevation range and species composition. ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Given is the attribute name, attribute type, the measurement unit and a brief description.  The forest cover type is the classification  problem.  The order of this listing corresponds to the order of numerals along the rows of the database.\\r\\n\\r\\nName / Data Type / Measurement / Description\\r\\n\\r\\nElevation / quantitative /meters / Elevation in meters\\r\\nAspect / quantitative / azimuth / Aspect in degrees azimuth\\r\\nSlope / quantitative / degrees / Slope in degrees\\r\\nHorizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features\\r\\nVertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features\\r\\nHorizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway\\r\\nHillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice\\r\\nHillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice\\r\\nHillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice\\r\\nHorizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points\\r\\nWilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\\r\\nSoil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation\\r\\nCover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation', 'citation': None}}\n",
            "                                  name     role     type demographic  \\\n",
            "0                            Elevation  Feature  Integer        None   \n",
            "1                               Aspect  Feature  Integer        None   \n",
            "2                                Slope  Feature  Integer        None   \n",
            "3     Horizontal_Distance_To_Hydrology  Feature  Integer        None   \n",
            "4       Vertical_Distance_To_Hydrology  Feature  Integer        None   \n",
            "5      Horizontal_Distance_To_Roadways  Feature  Integer        None   \n",
            "6                        Hillshade_9am  Feature  Integer        None   \n",
            "7                       Hillshade_Noon  Feature  Integer        None   \n",
            "8                        Hillshade_3pm  Feature  Integer        None   \n",
            "9   Horizontal_Distance_To_Fire_Points  Feature  Integer        None   \n",
            "10                     Wilderness_Area  Feature  Integer        None   \n",
            "11                          Soil_Type1  Feature  Integer        None   \n",
            "12                          Soil_Type2  Feature  Integer        None   \n",
            "13                          Soil_Type3  Feature  Integer        None   \n",
            "14                          Soil_Type4  Feature  Integer        None   \n",
            "15                          Soil_Type5  Feature  Integer        None   \n",
            "16                          Soil_Type6  Feature  Integer        None   \n",
            "17                          Soil_Type7  Feature  Integer        None   \n",
            "18                          Soil_Type8  Feature  Integer        None   \n",
            "19                          Soil_Type9  Feature  Integer        None   \n",
            "20                         Soil_Type10  Feature  Integer        None   \n",
            "21                         Soil_Type11  Feature  Integer        None   \n",
            "22                         Soil_Type12  Feature  Integer        None   \n",
            "23                         Soil_Type13  Feature  Integer        None   \n",
            "24                         Soil_Type14  Feature  Integer        None   \n",
            "25                         Soil_Type15  Feature  Integer        None   \n",
            "26                         Soil_Type16  Feature  Integer        None   \n",
            "27                         Soil_Type17  Feature  Integer        None   \n",
            "28                         Soil_Type18  Feature  Integer        None   \n",
            "29                         Soil_Type19  Feature  Integer        None   \n",
            "30                         Soil_Type20  Feature  Integer        None   \n",
            "31                         Soil_Type21  Feature  Integer        None   \n",
            "32                         Soil_Type22  Feature  Integer        None   \n",
            "33                         Soil_Type23  Feature  Integer        None   \n",
            "34                         Soil_Type24  Feature  Integer        None   \n",
            "35                         Soil_Type25  Feature  Integer        None   \n",
            "36                         Soil_Type26  Feature  Integer        None   \n",
            "37                         Soil_Type27  Feature  Integer        None   \n",
            "38                         Soil_Type28  Feature  Integer        None   \n",
            "39                         Soil_Type29  Feature  Integer        None   \n",
            "40                         Soil_Type30  Feature  Integer        None   \n",
            "41                         Soil_Type31  Feature  Integer        None   \n",
            "42                         Soil_Type32  Feature  Integer        None   \n",
            "43                         Soil_Type33  Feature  Integer        None   \n",
            "44                         Soil_Type34  Feature  Integer        None   \n",
            "45                         Soil_Type35  Feature  Integer        None   \n",
            "46                         Soil_Type36  Feature  Integer        None   \n",
            "47                         Soil_Type37  Feature  Integer        None   \n",
            "48                         Soil_Type38  Feature  Integer        None   \n",
            "49                         Soil_Type39  Feature  Integer        None   \n",
            "50                         Soil_Type40  Feature  Integer        None   \n",
            "51                          Cover_Type  Feature  Integer        None   \n",
            "\n",
            "   description units missing_values  \n",
            "0         None  None             no  \n",
            "1         None  None             no  \n",
            "2         None  None             no  \n",
            "3         None  None             no  \n",
            "4         None  None             no  \n",
            "5         None  None             no  \n",
            "6         None  None             no  \n",
            "7         None  None             no  \n",
            "8         None  None             no  \n",
            "9         None  None             no  \n",
            "10        None  None             no  \n",
            "11        None  None             no  \n",
            "12        None  None             no  \n",
            "13        None  None             no  \n",
            "14        None  None             no  \n",
            "15        None  None             no  \n",
            "16        None  None             no  \n",
            "17        None  None             no  \n",
            "18        None  None             no  \n",
            "19        None  None             no  \n",
            "20        None  None             no  \n",
            "21        None  None             no  \n",
            "22        None  None             no  \n",
            "23        None  None             no  \n",
            "24        None  None             no  \n",
            "25        None  None             no  \n",
            "26        None  None             no  \n",
            "27        None  None             no  \n",
            "28        None  None             no  \n",
            "29        None  None             no  \n",
            "30        None  None             no  \n",
            "31        None  None             no  \n",
            "32        None  None             no  \n",
            "33        None  None             no  \n",
            "34        None  None             no  \n",
            "35        None  None             no  \n",
            "36        None  None             no  \n",
            "37        None  None             no  \n",
            "38        None  None             no  \n",
            "39        None  None             no  \n",
            "40        None  None             no  \n",
            "41        None  None             no  \n",
            "42        None  None             no  \n",
            "43        None  None             no  \n",
            "44        None  None             no  \n",
            "45        None  None             no  \n",
            "46        None  None             no  \n",
            "47        None  None             no  \n",
            "48        None  None             no  \n",
            "49        None  None             no  \n",
            "50        None  None             no  \n",
            "51        None  None             no  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-4609dc62482a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#Onehot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0my_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_covertype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# Converting the label into a matrix form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mEncoded\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;34m\"y should be a 1d array, got an array of shape {} instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter Tuning\n",
        "We need to find the optimal hyper-parameters for each model (neural network, decision tree and neural decision tree) for each dataset we are looking at."
      ],
      "metadata": {
        "id": "xAssIf6T78ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deep Neural Decision Tree"
      ],
      "metadata": {
        "id": "KfYIpOqtfugL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Iris Dataset"
      ],
      "metadata": {
        "id": "piI8w-_sf1Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iris_hyper_tune():\n",
        "  iris_model = DNDT(num_classes = 3,num_features = 4, temperature = 0.2, learning_rate = 0.05, iters = 1200, epsilon = 0.1)\n",
        "  iris_model.fit(Iris_X_train,Iris_y_train, Iris_X_test, Iris_y_test)\n",
        "  iris_model.closeSession()\n",
        "\n",
        "iris_hyper_tune ()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77-rVOhJf0sn",
        "outputId": "9428959c-4263-44e9-8c70-8ac4523045ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 %\n",
            "Loss:  1.1283684\n",
            "Accuracy:  0.34285714285714286\n",
            "\n",
            "\n",
            "25 %\n",
            "Loss:  0.17426483\n",
            "Accuracy:  0.9619047619047619\n",
            "\n",
            "\n",
            "50 %\n",
            "Loss:  0.118160404\n",
            "Accuracy:  0.9619047619047619\n",
            "\n",
            "\n",
            "75 %\n",
            "Loss:  0.11770253\n",
            "Accuracy:  0.9619047619047619\n",
            "\n",
            "\n",
            "Final Test Accuracy:  0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network"
      ],
      "metadata": {
        "id": "2nYptcMpCfEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Iris Dataset"
      ],
      "metadata": {
        "id": "1SxisklpCqr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def iris_hyper_tune():\n",
        "  #So, excluding learning rate which is determined with ADAM, so we'll just find hyper parameters for\n",
        "  # epochs, number of hidden layers, units per layer, and dropout rate\n",
        "\n",
        "  #Epochs\n",
        "  max = [0]*3\n",
        "  for i in [15,20,25,30]:\n",
        "    iris_model_epoch = NeuralNetwork(2,[500,500],4,0.1,3)\n",
        "    iris_model_epoch.validationfitting(Iris_X_train,Iris_y_train, Iris_X_test, Iris_y_test, False, i)\n",
        "    current = iris_model_epoch.test(Iris_X_test,Iris_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of epochs to run is:\" ,max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Number of Hidden Layers\n",
        "  max = [0]*3\n",
        "  for i in [1,2,3,4]:\n",
        "    units_layer = [500]*i\n",
        "    iris_model_hidden_layers = NeuralNetwork(i,units_layer, 4,0.1,3)\n",
        "    iris_model_hidden_layers.validationfitting(Iris_X_train,Iris_y_train, Iris_X_test, Iris_y_test, False, 30)\n",
        "    current = iris_model_hidden_layers.test(Iris_X_test,Iris_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of hidden layers to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Units per Layer\n",
        "  max = [0]*3\n",
        "  for i in [100,300,500,1000]:\n",
        "    units_layer = [i]*3\n",
        "    iris_model_units = NeuralNetwork(3,units_layer, 4,0.1,3)\n",
        "    iris_model_units.validationfitting(Iris_X_train,Iris_y_train, Iris_X_test, Iris_y_test, False, 30)\n",
        "    current = iris_model_units.test(Iris_X_test,Iris_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of units per layer to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Dropout Rate\n",
        "  max = [0]*3\n",
        "  for i in [0,0.1,0.2,0.3]:\n",
        "    iris_model_dropout = NeuralNetwork(3, [500,500,500], 4, i, 3)\n",
        "    iris_model_dropout.validationfitting(Iris_X_train,Iris_y_train, Iris_X_test, Iris_y_test, False, 30)\n",
        "    current = iris_model_dropout.test(Iris_X_test,Iris_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best dropout rate to run is: \",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "\n",
        "\n",
        "iris_hyper_tune()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuxc20El8W2l",
        "outputId": "3bf9aa62-9624-43f5-d388-d6f290bfb9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 - 0s - loss: 0.4201 - accuracy: 0.9333 - 26ms/epoch - 13ms/step\n",
            "2/2 - 0s - loss: 0.3491 - accuracy: 0.9778 - 44ms/epoch - 22ms/step\n",
            "2/2 - 0s - loss: 0.3133 - accuracy: 1.0000 - 32ms/epoch - 16ms/step\n",
            "2/2 - 0s - loss: 0.2853 - accuracy: 0.9556 - 40ms/epoch - 20ms/step\n",
            "The best number of epochs to run is: 25 \n",
            " Accuracy:  1.0\n",
            "2/2 - 0s - loss: 0.2554 - accuracy: 1.0000 - 27ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.2641 - accuracy: 0.9778 - 31ms/epoch - 15ms/step\n",
            "2/2 - 0s - loss: 0.0653 - accuracy: 0.9778 - 28ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.0399 - accuracy: 1.0000 - 29ms/epoch - 15ms/step\n",
            "The best number of hidden layers to run is: 4 \n",
            " Accuracy:  1.0\n",
            "2/2 - 0s - loss: 0.1954 - accuracy: 0.9778 - 34ms/epoch - 17ms/step\n",
            "2/2 - 0s - loss: 0.0682 - accuracy: 1.0000 - 27ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.0618 - accuracy: 0.9778 - 51ms/epoch - 26ms/step\n",
            "2/2 - 0s - loss: 0.0733 - accuracy: 0.9556 - 30ms/epoch - 15ms/step\n",
            "The best number of units per layer to run is: 300 \n",
            " Accuracy:  1.0\n",
            "2/2 - 0s - loss: 0.0722 - accuracy: 0.9778 - 28ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.0652 - accuracy: 0.9778 - 29ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.0638 - accuracy: 0.9778 - 29ms/epoch - 14ms/step\n",
            "2/2 - 0s - loss: 0.0714 - accuracy: 0.9778 - 28ms/epoch - 14ms/step\n",
            "The best dropout rate to run is:  0.2 \n",
            " Accuracy:  0.9777777791023254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Haberman's Survival"
      ],
      "metadata": {
        "id": "CVZsVuPMiSsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def haber_hypertuning():\n",
        "  #So, excluding learning rate which is determined with ADAM, so we'll just find hyper parameters for\n",
        "  # epochs, number of hidden layers, units per layer, and dropout rate\n",
        "  print(\"Hyperparameter for Haberman's Survival\")\n",
        "  #Epochs\n",
        "  max = [0]*3\n",
        "  for i in [15,20,25,30]:\n",
        "    haber_model_epoch = NeuralNetwork(2,[500,500],3,0.1,2)\n",
        "    haber_model_epoch.validationfitting(Haber_X_train,Haber_y_train, Haber_X_test, Haber_y_test, False, i)\n",
        "    current = haber_model_epoch.test(Haber_X_test,Haber_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of epochs to run is:\" ,max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Number of Hidden Layers\n",
        "  max = [0]*3\n",
        "  for i in [1,2,3,4]:\n",
        "    units_layer = [500]*i\n",
        "    haber_model_hidden_layers = NeuralNetwork(i,units_layer, 3,0.1,2)\n",
        "    haber_model_hidden_layers.validationfitting(Haber_X_train,Haber_y_train, Haber_X_test, Haber_y_test, False, 30)\n",
        "    current = haber_model_hidden_layers.test(Haber_X_test,Haber_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of hidden layers to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Units per Layer\n",
        "  max = [0]*3\n",
        "  for i in [100,300,500,1000]:\n",
        "    units_layer = [i]*3\n",
        "    haber_model_units = NeuralNetwork(3,units_layer, 3,0.1,2)\n",
        "    haber_model_units.validationfitting(Haber_X_train,Haber_y_train, Haber_X_test, Haber_y_test, False, 30)\n",
        "    current = haber_model_units.test(Haber_X_test,Haber_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of units per layer to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Dropout Rate\n",
        "  max = [0]*3\n",
        "  for i in [0,0.1,0.2,0.3]:\n",
        "    haber_model_dropout = NeuralNetwork(3, [500,500,500], 3, i, 2)\n",
        "    haber_model_dropout.validationfitting(Haber_X_train,Haber_y_train, Haber_X_test, Haber_y_test, False, 30)\n",
        "    current = haber_model_dropout.test(Haber_X_test,Haber_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best dropout rate to run is: \",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "\n",
        "\n",
        "haber_hypertuning ()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGgPIcA5iQTl",
        "outputId": "71db84ab-498c-46ad-a983-eeeeb10b4567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter for Haberman's Survival\n",
            "3/3 - 0s - loss: 0.5375 - accuracy: 0.7500 - 28ms/epoch - 9ms/step\n",
            "3/3 - 0s - loss: 0.5402 - accuracy: 0.7500 - 29ms/epoch - 10ms/step\n",
            "3/3 - 0s - loss: 0.5253 - accuracy: 0.7609 - 33ms/epoch - 11ms/step\n",
            "3/3 - 0s - loss: 0.5503 - accuracy: 0.7500 - 31ms/epoch - 10ms/step\n",
            "The best number of epochs to run is: 25 \n",
            " Accuracy:  0.760869562625885\n",
            "3/3 - 0s - loss: 0.6102 - accuracy: 0.7391 - 31ms/epoch - 10ms/step\n",
            "3/3 - 0s - loss: 0.5990 - accuracy: 0.7500 - 32ms/epoch - 11ms/step\n",
            "3/3 - 0s - loss: 0.5904 - accuracy: 0.7500 - 32ms/epoch - 11ms/step\n",
            "3/3 - 0s - loss: 0.5935 - accuracy: 0.7500 - 32ms/epoch - 11ms/step\n",
            "The best number of hidden layers to run is: 3 \n",
            " Accuracy:  0.75\n",
            "3/3 - 0s - loss: 0.5960 - accuracy: 0.7391 - 44ms/epoch - 15ms/step\n",
            "3/3 - 0s - loss: 0.6028 - accuracy: 0.7500 - 38ms/epoch - 13ms/step\n",
            "3/3 - 0s - loss: 0.5922 - accuracy: 0.7174 - 32ms/epoch - 11ms/step\n",
            "3/3 - 0s - loss: 0.5755 - accuracy: 0.7174 - 35ms/epoch - 12ms/step\n",
            "The best number of units per layer to run is: 300 \n",
            " Accuracy:  0.75\n",
            "3/3 - 0s - loss: 0.6730 - accuracy: 0.7391 - 31ms/epoch - 10ms/step\n",
            "3/3 - 0s - loss: 0.5779 - accuracy: 0.7391 - 34ms/epoch - 11ms/step\n",
            "3/3 - 0s - loss: 0.5483 - accuracy: 0.7283 - 30ms/epoch - 10ms/step\n",
            "3/3 - 0s - loss: 0.5814 - accuracy: 0.7391 - 30ms/epoch - 10ms/step\n",
            "The best dropout rate to run is:  0.1 \n",
            " Accuracy:  0.739130437374115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Titanic"
      ],
      "metadata": {
        "id": "1gxLhKL0M6W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tit_hypertuning():\n",
        "  #So, excluding learning rate which is determined with ADAM, so we'll just find hyper parameters for\n",
        "  # epochs, number of hidden layers, units per layer, and dropout rate\n",
        "  print(\"Hyperparameters for Car Evaluation\")\n",
        "\n",
        "  #Epochs\n",
        "  max = [0]*3\n",
        "  for i in [15,20,25,30]:\n",
        "    tit_model_epoch = NeuralNetwork(2,[500,500],27,0.1,2)\n",
        "    tit_model_epoch.validationfitting(Tit_X_train,Tit_y_train, Tit_X_test, Tit_y_test, False, i)\n",
        "    current = tit_model_epoch.test(Tit_X_test,Tit_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of epochs to run is:\" ,max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Number of Hidden Layers\n",
        "  max = [0]*3\n",
        "  for i in [1,2,3,4]:\n",
        "    units_layer = [500]*i\n",
        "    tit_model_hidden_layers = NeuralNetwork(i,units_layer, 27,0.1,2)\n",
        "    tit_model_hidden_layers.validationfitting(Tit_X_train,Tit_y_train, Tit_X_test, Tit_y_test, False, 30)\n",
        "    current = tit_model_hidden_layers.test(Tit_X_test,Tit_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of hidden layers to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Units per Layer\n",
        "  max = [0]*3\n",
        "  for i in [100,300,500,1000]:\n",
        "    units_layer = [i]*3\n",
        "    tit_model_units = NeuralNetwork(3,units_layer, 27,0.1,2)\n",
        "    tit_model_units.validationfitting(Tit_X_train,Tit_y_train, Tit_X_test, Tit_y_test, False, 30)\n",
        "    current = tit_model_units.test(Tit_X_test,Tit_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best number of units per layer to run is:\",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "  #Dropout Rate\n",
        "  max = [0]*3\n",
        "  for i in [0,0.1,0.2,0.3]:\n",
        "    tit_model_dropout = NeuralNetwork(3, [500,500,500], 27, i, 2)\n",
        "    tit_model_dropout.validationfitting(Tit_X_train,Tit_y_train, Tit_X_test, Tit_y_test, False, 30)\n",
        "    current = tit_model_dropout.test(Tit_X_test,Tit_y_test)\n",
        "    if max[1] < current[1]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "    elif max[1] == current[1] and current[0] < max[2]:\n",
        "      max[0] = i\n",
        "      max[1] = current[1]\n",
        "      max[2] = current[0]\n",
        "  print(\"The best dropout rate to run is: \",max[0],\"\\n Accuracy: \",max[1])\n",
        "\n",
        "\n",
        "\n",
        "tit_hypertuning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wMYHZTMM9uo",
        "outputId": "232e3a5f-f852-407d-8b85-10fe7b19e17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters for Car Evaluation\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 49ms/epoch - 4ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 47ms/epoch - 4ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 44ms/epoch - 3ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 41ms/epoch - 3ms/step\n",
            "The best number of epochs to run is: 15 \n",
            " Accuracy:  0.7480915784835815\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 61ms/epoch - 5ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 42ms/epoch - 3ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 51ms/epoch - 4ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 90ms/epoch - 7ms/step\n",
            "The best number of hidden layers to run is: 1 \n",
            " Accuracy:  0.7480915784835815\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 41ms/epoch - 3ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 45ms/epoch - 3ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 74ms/epoch - 6ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 81ms/epoch - 6ms/step\n",
            "The best number of units per layer to run is: 100 \n",
            " Accuracy:  0.7480915784835815\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 53ms/epoch - 4ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 61ms/epoch - 5ms/step\n",
            "13/13 - 0s - loss: nan - accuracy: 0.7481 - 48ms/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "By1BIXCDQvmG"
      }
    }
  ]
}